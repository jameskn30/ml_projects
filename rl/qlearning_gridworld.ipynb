{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.28.4, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pygame\n",
    "import sys\n",
    "import time\n",
    "\n",
    "colors = {\n",
    "    \"black\": (0,0,0),\n",
    "    \"white\": (255,255,255),\n",
    "    \"light_white\": (200,200,200),\n",
    "    \"blue\": (0,0,255),\n",
    "    \"green\": (51, 204, 51),\n",
    "}\n",
    "class GridWorldRenderer():\n",
    "    def __init__(self, rows, columns, cell_size = 50):\n",
    "        self.rows = rows \n",
    "        self.columns = columns\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "    def start(self, state):\n",
    "        pygame.init()\n",
    "        # self.test = False \n",
    "        # self.test_image = False \n",
    "\n",
    "        self._window_width = self.columns * self.cell_size\n",
    "        self._window_height = self.rows * self.cell_size\n",
    "\n",
    "        self.screen = pygame.display.set_mode((self._window_width, self._window_height))\n",
    "        pygame.display.set_caption(f\"Grid World {self.rows}x{self.columns}\" )\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.screen.fill(colors['white'])\n",
    "\n",
    "        self.border_color = colors['black']\n",
    "        self.state = state\n",
    "\n",
    "        self.update(state)\n",
    "\n",
    "    # def _drawgrid(self):\n",
    "    #     for i, x in enumerate(range(0, self._window_width, self.cell_size)):\n",
    "    #         for j, y in enumerate(range(0, self._window_height, self.cell_size)):\n",
    "    #             color = colors['white']\n",
    "\n",
    "    #             rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "\n",
    "    #             pygame.draw.rect(self.screen, color, rect)\n",
    "    #             if self.test_image and i == 0 and j == 0 :\n",
    "    #                 self.draw_robot(self.screen, x,y)\n",
    "    #             if i == 1 and j == 0 :\n",
    "    #                 self.draw_battery(self.screen, x,y)\n",
    "    #             if i == 0 and j == 1 :\n",
    "    #                 self.draw_crap(self.screen, x,y)\n",
    "\n",
    "    #             border = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "    #             pygame.draw.rect(self.screen, self.border_color, border, 1)\n",
    "\n",
    "    def _drawstate(self):\n",
    "        for i, x in enumerate(range(0, self._window_width, self.cell_size)):\n",
    "            for j, y in enumerate(range(0, self._window_height, self.cell_size)):\n",
    "                if self.state[i][j] == 1:\n",
    "                    rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "                    blue = colors['blue']\n",
    "                    pygame.draw.rect(self.screen, blue, rect)\n",
    "                else:\n",
    "                    rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "                    white = colors['white']\n",
    "                    pygame.draw.rect(self.screen, white, rect)\n",
    "\n",
    "                black = colors['black']\n",
    "                pygame.draw.rect(self.screen, black, rect, 1)\n",
    "                \n",
    "\n",
    "    # def toggle(self):\n",
    "    #     self.test = not self.test\n",
    "    \n",
    "    # def _draw_object(self, screen, img, x,y):\n",
    "    #     img = pygame.image.load(img)\n",
    "    #     img = pygame.transform.scale(img, (self.cell_size, self.cell_size))\n",
    "    #     screen.blit(img, (x,y))\n",
    "    #     border = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "    #     pygame.draw.rect(self.screen, self.border_color, border, 1)\n",
    "\n",
    "    # def draw_robot(self, screen, x, y):\n",
    "    #     robot_img = './assets/robot.jpg'\n",
    "    #     self._draw_object(screen, robot_img, x, y)\n",
    "\n",
    "    # def draw_crap(self, screen, x, y):\n",
    "    #     img = './assets/crap.png'\n",
    "    #     self._draw_object(screen, img, x, y)\n",
    "\n",
    "    # def draw_battery(self, screen, x, y):\n",
    "    #     img = './assets/battery.png'\n",
    "    #     self._draw_object(screen, img, x, y)\n",
    "    \n",
    "    def update(self, new_state):\n",
    "        #clear screen\n",
    "        self.clock.tick(60)\n",
    "        self.screen.fill(colors['white'])\n",
    "        \n",
    "        self.state = new_state\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.end()\n",
    "            \n",
    "\n",
    "        self._drawstate()\n",
    "        pygame.display.update()\n",
    "        \n",
    "    def end(self):\n",
    "        print('exit')\n",
    "        pygame.quit()\n",
    "        sys.exit()\n",
    "\n",
    "    \n",
    "    #call a while  loop to update pygame drawing\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "# gridworld = GridWorldRenderer(8,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAlgorithm():\n",
    "    def __init__(self, env, **kwargs: dict) -> None:\n",
    "        self.n_training_eps = int(self._get(kwargs, \"n_training_eps\", 10000))\n",
    "        self.n_eval_eps = int(self._get(kwargs, \"n_eval_eps\", 100))\n",
    "        self.max_steps = int(self._get(kwargs, \"max_steps\", 99))\n",
    "        self.learning_rate = float(self._get(kwargs, \"learning_rate\", 0.001))\n",
    "        self.max_epsilon = float(self._get(kwargs, \"max_epsilon\", 1.0))\n",
    "        self.min_epsilon = float(self._get(kwargs, \"min_epsilon\", 0.005))\n",
    "        self.decay_rate = float(self._get(kwargs, \"decay_rate\", 0.0005))\n",
    "        self.gamma = float(self._get(kwargs, \"gamma\", 0.95))\n",
    "        self.env = env\n",
    "        self._reset_qtable()\n",
    "    \n",
    "    def _reset_qtable(self):\n",
    "        self.qtable = self._init_qtable(self.env.observation_space.n, self.env.action_space.n)\n",
    "\n",
    "    def __str__(self):\n",
    "        print(\"=\" * 20)\n",
    "        print(f\"Q-TABLE, shape = {self.qtable.shape}\")\n",
    "        print(\"=\" * 20)\n",
    "        print(self.qtable)\n",
    "        print(\"=\" * 20)\n",
    "        print(\"PARAMS\")\n",
    "        print(\"=\" * 20)\n",
    "        print(f'n_training_eps = {self.n_training_eps}')\n",
    "        print(f'n_eval_eps = {self.n_eval_eps}')\n",
    "        print(f'max_steps = {self.max_steps}')\n",
    "        print(f'learning_rate = {self.learning_rate}')\n",
    "        print(f'max_epsilon = {self.max_epsilon}')\n",
    "        print(f'min_epsilon = {self.min_epsilon}')\n",
    "        print(f'decay rate = {self.decay_rate}')\n",
    "        print(f'gamma = {self.gamma}')\n",
    "        print(f'env = {self.env}')\n",
    "        return ''\n",
    "\n",
    "    def _get(self, dict, key, default):\n",
    "        return dict[key] if key in dict else default\n",
    "\n",
    "    def _init_qtable(self, state_space,action_space):\n",
    "        qtable = np.zeros((state_space, action_space))\n",
    "        return qtable\n",
    "    \n",
    "    def _epsilon_greedy_policy(self, state, epsilon):\n",
    "        random_init = random.uniform(-1,1)\n",
    "        if random_init > epsilon:\n",
    "            action = np.argmax(self.qtable[state])\n",
    "        else:\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        loop = tqdm(list(range(self.n_training_eps)))\n",
    "        #reset qtable\n",
    "        self._reset_qtable()\n",
    "\n",
    "        print(\"=\" * 20)\n",
    "        print(\"TRAINING\")\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "        for ep in loop:\n",
    "            epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_rate * ep)\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            for step in range(self.max_steps):\n",
    "                action = self._epsilon_greedy_policy(state, epsilon)\n",
    "                new_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                self.qtable[state][action] = self.qtable[state][action] + \\\n",
    "                    self.learning_rate * (reward  + self.gamma * np.max(self.qtable[new_state]) - self.qtable[state][action])\n",
    "                \n",
    "                state = new_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            loop.set_description(f\"ep = {ep}, eposilon = {epsilon:.2f}\")\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return np.argmax(self.qtable[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_map():\n",
    "    return 'SFFH\\nH'\n",
    "\n",
    "def GET_MAP(map):\n",
    "    if map == \"4x4\": \n",
    "        return [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ]\n",
    "\n",
    "    if map == \"8x8\": \n",
    "        return [\n",
    "            \"SFFFFFFF\",\n",
    "            \"FFFFFFFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FFFFFHFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FHHFFFHF\",\n",
    "            \"FHFFHFHF\",\n",
    "            \"FFFHFFFG\",\n",
    "        ],\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self, map, reward_dict, **kwargs):\n",
    "\n",
    "        #Check if map has the same dimension as rows and cols\n",
    "        rows, cols, map = self._process_map(map)\n",
    "\n",
    "        self.map = map\n",
    "        nS = rows * cols\n",
    "        nA = 4\n",
    "        self.rows, self.cols = rows, cols\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [rows - 1, cols - 1]\n",
    "\n",
    "        #save reward dict\n",
    "        self.reward_dict = reward_dict\n",
    "\n",
    "        #default state\n",
    "        self.state = np.zeros((self.rows, self.cols))\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "\n",
    "        self.observation_space = spaces.Discrete(nS)\n",
    "        self.action_space = spaces.Discrete(nA)\n",
    "\n",
    "        #time step used to track how long this agent performing\n",
    "        #if it's to long, terminate early\n",
    "        self.timestep = 0\n",
    "        self.max_timestep = int(kwargs['max_timestep']) if kwargs['max_timestep'] != None else 100\n",
    "\n",
    "        #termination status\n",
    "        self.terminated = False\n",
    "\n",
    "        #render using pygame\n",
    "        self.renderer = GridWorldRenderer(rows, cols)\n",
    "    \n",
    "    #Reset environmentj\n",
    "    def reset(self):\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [self.rows - 1, self.cols - 1]\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "        self.timestep = 0\n",
    "        return 0\n",
    "\n",
    "    def _process_map(self, map_data):\n",
    "        rows = map_data\n",
    "        rows_n = len(rows)\n",
    "        cols_n = len(rows[0])\n",
    "        map = [['' for _ in range(cols_n)] for _ in range(rows_n)]\n",
    "        for i, row in enumerate(rows):\n",
    "            for j, val in enumerate(row):\n",
    "                map[i][j] = val \n",
    "\n",
    "        return rows_n, cols_n, map\n",
    "\n",
    "    def set_state(self, agent_pos, goal_pos):\n",
    "        self.state = np.zeros((self.rows, self.cols))\n",
    "        self.state[tuple(agent_pos)] = 1\n",
    "        self.state[tuple(goal_pos)] = 0.5\n",
    "        observation = self.state.flatten()\n",
    "        return observation\n",
    "    \n",
    "    def reached_goal(self, pos):\n",
    "        x,y = pos\n",
    "        return True if self.map[x][y] == 'G'else False\n",
    "    \n",
    "    def get_reward(self, pos):\n",
    "        x,y = pos\n",
    "        val = self.map[x][y]\n",
    "        return self.reward_dict[val] if val in self.reward_dict else 0\n",
    "\n",
    "    #Step function: agent take step in env\n",
    "    def step(self, action):\n",
    "        #actions:\n",
    "        #0: down\n",
    "        #1:up \n",
    "        #2:right\n",
    "        #3:left\n",
    "\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] += 1\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] -= 1\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] += 1\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] -= 1\n",
    "\n",
    "        #clip the agent position to avoid out of bound\n",
    "        self.agent_pos[0] = np.clip(self.agent_pos[0], 0, self.cols - 1)\n",
    "        self.agent_pos[1] = np.clip(self.agent_pos[1], 0, self.rows - 1)\n",
    "\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "        observation = self.state.flatten()\n",
    "\n",
    "        #Check if the agent takes too long to go to goal\n",
    "        self.timestep += 1\n",
    "        self.terminated = True if self.timestep > self.max_timestep else False\n",
    "\n",
    "        #Define your reward function\n",
    "        reward = self.get_reward(self.agent_pos)\n",
    "\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
    "            self.terminated = True\n",
    "            reward = 1\n",
    "        \n",
    "        if self.reached_goal(self.agent_pos):\n",
    "            self.terminated = True\n",
    "        \n",
    "        info = {}\n",
    "        #return:\n",
    "        #next state, argmax to get the new state of agent, np.argmax([0,0,1,0,0,0,0.5]) = 2\n",
    "        #reward\n",
    "        #done or not\n",
    "        #extra infomation\n",
    "        return np.argmax(observation), reward, self.terminated, info\n",
    "    \n",
    "    def render(self, agent: QLearningAlgorithm) -> None:\n",
    "        #Put a renderer here\n",
    "        self.reset()\n",
    "        self.renderer.start(self.state)\n",
    "        max_iter = 100\n",
    "        iter =0 \n",
    "        curr_state = 0\n",
    "        while self.terminated == False and iter < max_iter:\n",
    "            self.renderer.update(self.state)\n",
    "            observation = self.state.flatten()\n",
    "            curr_state = np.argmax(observation)\n",
    "            action = agent.get_action(curr_state)\n",
    "\n",
    "            curr_state, _,_,_ = self.step(action)\n",
    "            pygame.time.wait(500)\n",
    "            iter += 1\n",
    "        self.renderer.end()\n",
    "        print(\"Terminated\")\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        print(\"=\" * 20)\n",
    "        print('ENV MAP')\n",
    "        print(\"=\" * 20)\n",
    "        for row in self.map:\n",
    "            print(row)\n",
    "        print(\"=\" * 20)\n",
    "        print('ENV STATE')\n",
    "        print(\"=\" * 20)\n",
    "        print(self.state)\n",
    "        print(\"=\" * 20)\n",
    "        print(\"ENV DESCRIPTION\")\n",
    "        print(\"=\" * 20)\n",
    "        print(f'reward dict = {self.reward_dict}')\n",
    "        print(f'timestep = {self.timestep}, max_timestep = {self.max_timestep},')\n",
    "        print(f'obs space = {self.observation_space.n}')\n",
    "        print(f'actions = {self.action_space.n}')\n",
    "        print(f'agent position = {self.agent_pos}')\n",
    "        print(f'goal position = {self.goal_pos}')\n",
    "        print(f'terminated = {self.terminated}')\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent import QLearningAgent\n",
    "from src.env import GridWorldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\ML\\ml_projects\\rl\\qlearning_gridworld.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\",is_slippery=False)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m reward_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mG\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m }\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m env \u001b[39m=\u001b[39m GridWorldEnv(\u001b[39m'\u001b[39;49m\u001b[39m4x4\u001b[39;49m\u001b[39m'\u001b[39;49m, reward_dict \u001b[39m=\u001b[39;49m reward_dict, max_timestep \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32me:\\ML\\ml_projects\\rl\\src\\env.py:72\u001b[0m, in \u001b[0;36mGridWorldEnv.__init__\u001b[1;34m(self, map, reward_dict, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterminated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m#render using pygame\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrenderer \u001b[39m=\u001b[39m GridWorldRenderer(rows, cols)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'map'"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\",is_slippery=False)\n",
    "reward_dict = {\n",
    "    'G': 1,\n",
    "}\n",
    "env = GridWorldEnv('4x4', reward_dict = reward_dict, max_timestep = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep = 89, eposilon = 0.96:   0%|          | 50/10000 [00:00<00:19, 497.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "TRAINING\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep = 9999, eposilon = 0.01: 100%|██████████| 10000/10000 [00:16<00:00, 619.30it/s]\n"
     ]
    }
   ],
   "source": [
    "qlearning = QLearningAgent(env)\n",
    "# print(qlearning)\n",
    "qlearning.train()\n",
    "qlearning.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = GridWorldEnv('4x4', reward_dict = reward_dict, max_timestep = 10)\n",
    "env.render(qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Q-TABLE, shape = (16, 4)\n",
      "====================\n",
      "[[0.09949599 0.11322295 0.36305579 0.12115583]\n",
      " [0.47276961 0.15376244 0.0712842  0.10412594]\n",
      " [0.21747489 0.01687942 0.0089549  0.04121438]\n",
      " [0.08598744 0.00277095 0.00310443 0.00603573]\n",
      " [0.03180685 0.04334721 0.30378327 0.03944882]\n",
      " [0.60242254 0.14529019 0.22189815 0.09074389]\n",
      " [0.57151982 0.02734006 0.10009132 0.09252812]\n",
      " [0.50228571 0.00580963 0.04459946 0.05191821]\n",
      " [0.00381836 0.01686956 0.26355942 0.01684962]\n",
      " [0.1245455  0.19352862 0.73239184 0.07149765]\n",
      " [0.35437275 0.17116539 0.86928223 0.25285425]\n",
      " [0.97256123 0.14824445 0.40984399 0.2756309 ]\n",
      " [0.00123246 0.00506839 0.05341668 0.00131691]\n",
      " [0.03208811 0.06096813 0.36581754 0.00425051]\n",
      " [0.12264679 0.12525629 0.76723571 0.03907166]\n",
      " [0.         0.         0.         0.        ]]\n",
      "====================\n",
      "PARAMS\n",
      "====================\n",
      "n_training_eps = 10000\n",
      "n_eval_eps = 100\n",
      "max_steps = 99\n",
      "learning_rate = 0.001\n",
      "max_epsilon = 1.0\n",
      "min_epsilon = 0.005\n",
      "decay rate = 0.0005\n",
      "gamma = 0.95\n",
      "====================\n",
      "ENV MAP\n",
      "====================\n",
      "['S', 'F', 'F', 'F']\n",
      "['F', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "====================\n",
      "ENV STATE\n",
      "====================\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "ENV DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 1}\n",
      "timestep = 6, max_timestep = 10,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [3, 3]\n",
      "goal position = [3, 3]\n",
      "terminated = True\n",
      "env = \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearning.qtable\n",
    "#save qtable\n",
    "with open('qtable.npy', 'wb') as file:\n",
    "    np.save(file, qlearning.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.95689949e-01 1.38064714e-01 1.06121211e-01 1.35516080e-01]\n",
      " [3.18065017e-01 3.82771463e-02 6.15608108e-03 4.34399335e-02]\n",
      " [5.39815032e-02 1.68079049e-03 2.32974938e-04 9.53439141e-03]\n",
      " [5.84049610e-03 6.89223099e-05 6.28668322e-05 3.17291475e-04]\n",
      " [1.44302505e-01 1.20382128e-01 5.09127587e-01 1.66794701e-01]\n",
      " [6.42008184e-01 9.20341031e-02 1.17422646e-01 1.66434772e-01]\n",
      " [3.29896150e-01 4.05492912e-03 1.06120522e-02 6.45226069e-02]\n",
      " [8.74166558e-02 1.67897546e-04 3.32167255e-03 7.29796541e-03]\n",
      " [5.84698450e-02 5.28496164e-02 4.20337201e-01 4.89881527e-02]\n",
      " [7.74774138e-01 2.30768245e-01 2.95454158e-01 1.24687229e-01]\n",
      " [7.21301622e-01 4.26779797e-02 1.00492352e-01 1.30932674e-01]\n",
      " [4.38019536e-01 3.81403863e-03 2.98114722e-02 3.52019758e-02]\n",
      " [2.77711916e-02 2.98212636e-02 3.99026040e-01 3.47768298e-02]\n",
      " [3.79940201e-01 2.80023416e-01 8.91075089e-01 1.22651781e-01]\n",
      " [4.78308328e-01 2.60349632e-01 9.84378075e-01 3.37625110e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#load qtable\n",
    "with open('qtable.npy', 'rb') as file:\n",
    "    qtable = np.load(file)\n",
    "\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(a[(0,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "ENV MAP\n",
      "====================\n",
      "['S', 'F', 'F', 'F']\n",
      "['F', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "====================\n",
      "ENV STATE\n",
      "====================\n",
      "[[1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "ENV DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 1}\n",
      "timestep = 0, max_timestep = 10,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [0, 0]\n",
      "goal position = [3, 3]\n",
      "terminated = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reward_dict = {\n",
    "    'G': 1,\n",
    "}\n",
    "env = GridWorldEnv(map = GET_MAP('4x4'), reward_dict = reward_dict, max_timestep = 10)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "env.render(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Renderere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.env import GridWorldRenderer\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69385331786071\n"
     ]
    }
   ],
   "source": [
    "print(random.uniform(0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\ML\\ml_projects\\rl\\src\n",
      "exit\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "map = [\n",
    "    ['A', 'R', '', 'S'],\n",
    "    ['', '', '', ''],\n",
    "    ['B', '', 'B', 'R'],\n",
    "    ['', '', '', 'G']\n",
    "    ]\n",
    "\n",
    "rows = 10\n",
    "cols = 10\n",
    "\n",
    "def gen_obj():\n",
    "    possible_choice = ['', 'R', 'S', 'B']\n",
    "    chance =  random.uniform(0.0,1.0)\n",
    "    if 0.0 < chance < 0.1: return 'S'\n",
    "    if 0.1 < chance < 0.2: return 'R'\n",
    "    if 0.2 < chance < 0.3: return 'B'\n",
    "    else: return ''\n",
    "\n",
    "map = [[gen_obj() for _ in range(rows)] for _ in range(cols)]\n",
    "map[0][0] = 'A'\n",
    "map[rows - 1][cols - 1] = 'G'\n",
    "\n",
    "renderer = GridWorldRenderer(10,10, map = map)\n",
    "renderer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
