{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame\n",
    "import sys\n",
    "import time\n",
    "\n",
    "colors = {\n",
    "    \"black\": (0,0,0),\n",
    "    \"white\": (255,255,255),\n",
    "    \"light_white\": (200,200,200),\n",
    "    \"blue\": (0,0,255),\n",
    "    \"green\": (51, 204, 51),\n",
    "}\n",
    "class GridWorldRenderer():\n",
    "    def __init__(self, rows, columns, cell_size = 50):\n",
    "        self.rows = rows \n",
    "        self.columns = columns\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "    def start(self, state):\n",
    "        pygame.init()\n",
    "        # self.test = False \n",
    "        # self.test_image = False \n",
    "\n",
    "        self._window_width = self.columns * self.cell_size\n",
    "        self._window_height = self.rows * self.cell_size\n",
    "\n",
    "        self.screen = pygame.display.set_mode((self._window_width, self._window_height))\n",
    "        pygame.display.set_caption(f\"Grid World {self.rows}x{self.columns}\" )\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.screen.fill(colors['white'])\n",
    "\n",
    "        self.border_color = colors['black']\n",
    "        self.state = state\n",
    "\n",
    "        self.update(state)\n",
    "\n",
    "    # def _drawgrid(self):\n",
    "    #     for i, x in enumerate(range(0, self._window_width, self.cell_size)):\n",
    "    #         for j, y in enumerate(range(0, self._window_height, self.cell_size)):\n",
    "    #             color = colors['white']\n",
    "\n",
    "    #             rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "\n",
    "    #             pygame.draw.rect(self.screen, color, rect)\n",
    "    #             if self.test_image and i == 0 and j == 0 :\n",
    "    #                 self.draw_robot(self.screen, x,y)\n",
    "    #             if i == 1 and j == 0 :\n",
    "    #                 self.draw_battery(self.screen, x,y)\n",
    "    #             if i == 0 and j == 1 :\n",
    "    #                 self.draw_crap(self.screen, x,y)\n",
    "\n",
    "    #             border = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "    #             pygame.draw.rect(self.screen, self.border_color, border, 1)\n",
    "\n",
    "    def _drawstate(self):\n",
    "        for i, x in enumerate(range(0, self._window_width, self.cell_size)):\n",
    "            for j, y in enumerate(range(0, self._window_height, self.cell_size)):\n",
    "                if self.state[i][j] == 1:\n",
    "                    rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "                    blue = colors['blue']\n",
    "                    pygame.draw.rect(self.screen, blue, rect)\n",
    "                else:\n",
    "                    rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "                    white = colors['white']\n",
    "                    pygame.draw.rect(self.screen, white, rect)\n",
    "\n",
    "                black = colors['black']\n",
    "                pygame.draw.rect(self.screen, self.border_color, rect, 1)\n",
    "                \n",
    "    # def toggle(self):\n",
    "    #     self.test = not self.test\n",
    "    \n",
    "    # def _draw_object(self, screen, img, x,y):\n",
    "    #     img = pygame.image.load(img)\n",
    "    #     img = pygame.transform.scale(img, (self.cell_size, self.cell_size))\n",
    "    #     screen.blit(img, (x,y))\n",
    "    #     border = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "    #     pygame.draw.rect(self.screen, self.border_color, border, 1)\n",
    "\n",
    "    # def draw_robot(self, screen, x, y):\n",
    "    #     robot_img = './assets/robot.jpg'\n",
    "    #     self._draw_object(screen, robot_img, x, y)\n",
    "\n",
    "    # def draw_crap(self, screen, x, y):\n",
    "    #     img = './assets/crap.png'\n",
    "    #     self._draw_object(screen, img, x, y)\n",
    "\n",
    "    # def draw_battery(self, screen, x, y):\n",
    "    #     img = './assets/battery.png'\n",
    "    #     self._draw_object(screen, img, x, y)\n",
    "    \n",
    "    def update(self, new_state):\n",
    "        #clear screen\n",
    "        self.clock.tick(60)\n",
    "        self.screen.fill(colors['white'])\n",
    "        \n",
    "        self.state = new_state\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.end()\n",
    "            \n",
    "            # if event.type == pygame.KEYDOWN:\n",
    "            #     self.test_image = not self.test_image\n",
    "\n",
    "        self._drawstate()\n",
    "        pygame.display.update()\n",
    "        \n",
    "    def end(self):\n",
    "        print('exit')\n",
    "        pygame.quit()\n",
    "        sys.exit()\n",
    "\n",
    "    \n",
    "    #call a while  loop to update pygame drawing\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "# gridworld = GridWorldRenderer(8,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     gridworld.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = [-1, 0]\n",
    "\n",
    "a = np.clip(a, 0, 2)\n",
    "print(a)\n",
    "\n",
    "print(np.all((a >=0) & (a <= 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array([0,0,0,1,0,0,0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_map():\n",
    "    return 'SFFH\\nH'\n",
    "\n",
    "def GET_MAP(map):\n",
    "    if map == \"4x4\": \n",
    "        return [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ]\n",
    "\n",
    "    if map == \"8x8\": \n",
    "        return [\n",
    "            \"SFFFFFFF\",\n",
    "            \"FFFFFFFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FFFFFHFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FHHFFFHF\",\n",
    "            \"FHFFHFHF\",\n",
    "            \"FFFHFFFG\",\n",
    "        ],\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self, map, reward_dict, **kwargs):\n",
    "\n",
    "        #Check if map has the same dimension as rows and cols\n",
    "        rows, cols, map = self._process_map(map)\n",
    "\n",
    "        self.map = map\n",
    "        nS = rows * cols\n",
    "        nA = 4\n",
    "        self.rows, self.cols = rows, cols\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [rows - 1, cols - 1]\n",
    "\n",
    "        #save reward dict\n",
    "        self.reward_dict = reward_dict\n",
    "\n",
    "        #default state\n",
    "        self.state = np.zeros((self.rows, self.cols))\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "\n",
    "        self.observation_space = spaces.Discrete(nS)\n",
    "        self.action_space = spaces.Discrete(nA)\n",
    "\n",
    "        #time step used to track how long this agent performing\n",
    "        #if it's to long, terminate early\n",
    "        self.timestep = 0\n",
    "        self.max_timestep = int(kwargs['max_timestep']) if kwargs['max_timestep'] != None else 100\n",
    "\n",
    "        #termination status\n",
    "        self.terminated = False\n",
    "\n",
    "        #render using pygame\n",
    "        self.renderer = GridWorldRenderer(rows, cols)\n",
    "    \n",
    "    #Reset environmentj\n",
    "    def reset(self):\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [self.rows - 1, self.cols - 1]\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "        self.timestep = 0\n",
    "        return 0\n",
    "\n",
    "    def _process_map(self, map_data):\n",
    "        rows = map_data\n",
    "        rows_n = len(rows)\n",
    "        cols_n = len(rows[0])\n",
    "        map = [['' for _ in range(cols_n)] for _ in range(rows_n)]\n",
    "        for i, row in enumerate(rows):\n",
    "            for j, val in enumerate(row):\n",
    "                map[i][j] = val \n",
    "\n",
    "        return rows_n, cols_n, map\n",
    "\n",
    "    def set_state(self, agent_pos, goal_pos):\n",
    "        self.state = np.zeros((self.rows, self.cols))\n",
    "        self.state[tuple(agent_pos)] = 1\n",
    "        self.state[tuple(goal_pos)] = 0.5\n",
    "        observation = self.state.flatten()\n",
    "        return observation\n",
    "    \n",
    "    def reached_goal(self, pos):\n",
    "        x,y = pos\n",
    "        return True if self.map[x][y] == 'G'else False\n",
    "    \n",
    "    def get_reward(self, pos):\n",
    "        x,y = pos\n",
    "        val = self.map[x][y]\n",
    "        return self.reward_dict[val] if val in self.reward_dict else 0\n",
    "\n",
    "    #Step function: agent take step in env\n",
    "    def step(self, action):\n",
    "        #actions:\n",
    "        #0: down\n",
    "        #1:up \n",
    "        #2:right\n",
    "        #3:left\n",
    "\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] += 1\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] -= 1\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] += 1\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] -= 1\n",
    "\n",
    "        #clip the agent position to avoid out of bound\n",
    "        self.agent_pos[0] = np.clip(self.agent_pos[0], 0, self.cols - 1)\n",
    "        self.agent_pos[1] = np.clip(self.agent_pos[1], 0, self.rows - 1)\n",
    "\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "        observation = self.state.flatten()\n",
    "\n",
    "        #Check if the agent takes too long to go to goal\n",
    "        self.timestep += 1\n",
    "        self.terminated = True if self.timestep > self.max_timestep else False\n",
    "\n",
    "        #Define your reward function\n",
    "        reward = self.get_reward(self.agent_pos)\n",
    "\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
    "            self.terminated = True\n",
    "            reward = 1\n",
    "        \n",
    "        if self.reached_goal(self.agent_pos):\n",
    "            self.terminated = True\n",
    "        \n",
    "        info = {}\n",
    "        #return:\n",
    "        #next state, argmax to get the new state of agent, np.argmax([0,0,1,0,0,0,0.5]) = 2\n",
    "        #reward\n",
    "        #done or not\n",
    "        #extra infomation\n",
    "        return np.argmax(observation), reward, self.terminated, info\n",
    "    \n",
    "    def render(self, qtable):\n",
    "        #Put a renderer here\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        print(\"=\" * 20)\n",
    "        print('ENV MAP')\n",
    "        print(\"=\" * 20)\n",
    "        for row in self.map:\n",
    "            print(row)\n",
    "        print(\"=\" * 20)\n",
    "        print('ENV STATE')\n",
    "        print(\"=\" * 20)\n",
    "        print(self.state)\n",
    "        print(\"=\" * 20)\n",
    "        print(\"ENV DESCRIPTION\")\n",
    "        print(\"=\" * 20)\n",
    "        print(f'reward dict = {self.reward_dict}')\n",
    "        print(f'timestep = {self.timestep}, max_timestep = {self.max_timestep},')\n",
    "        print(f'obs space = {self.observation_space.n}')\n",
    "        print(f'actions = {self.action_space.n}')\n",
    "        print(f'agent position = {self.agent_pos}')\n",
    "        print(f'goal position = {self.goal_pos}')\n",
    "        print(f'terminated = {self.terminated}')\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAlgorithm():\n",
    "    def __init__(self, env, **kwargs: dict) -> None:\n",
    "        self.n_training_eps = int(self._get(kwargs, \"n_training_eps\", 10000))\n",
    "        self.n_eval_eps = int(self._get(kwargs, \"n_eval_eps\", 100))\n",
    "        self.max_steps = int(self._get(kwargs, \"max_steps\", 99))\n",
    "        self.learning_rate = float(self._get(kwargs, \"learning_rate\", 0.001))\n",
    "        self.max_epsilon = float(self._get(kwargs, \"max_epsilon\", 1.0))\n",
    "        self.min_epsilon = float(self._get(kwargs, \"min_epsilon\", 0.005))\n",
    "        self.decay_rate = float(self._get(kwargs, \"decay_rate\", 0.0005))\n",
    "        self.gamma = float(self._get(kwargs, \"gamma\", 0.95))\n",
    "        self.env = env\n",
    "        self._reset_qtable()\n",
    "    \n",
    "    def _reset_qtable(self):\n",
    "        self.qtable = self._init_qtable(self.env.observation_space.n, self.env.action_space.n)\n",
    "\n",
    "    def __str__(self):\n",
    "        print(\"=\" * 20)\n",
    "        print(f\"Q-TABLE, shape = {self.qtable.shape}\")\n",
    "        print(\"=\" * 20)\n",
    "        print(self.qtable)\n",
    "        print(\"=\" * 20)\n",
    "        print(\"PARAMS\")\n",
    "        print(\"=\" * 20)\n",
    "        print(f'n_training_eps = {self.n_training_eps}')\n",
    "        print(f'n_eval_eps = {self.n_eval_eps}')\n",
    "        print(f'max_steps = {self.max_steps}')\n",
    "        print(f'learning_rate = {self.learning_rate}')\n",
    "        print(f'max_epsilon = {self.max_epsilon}')\n",
    "        print(f'min_epsilon = {self.min_epsilon}')\n",
    "        print(f'decay rate = {self.decay_rate}')\n",
    "        print(f'gamma = {self.gamma}')\n",
    "        print(f'env = {self.env}')\n",
    "        return ''\n",
    "\n",
    "    def _get(self, dict, key, default):\n",
    "        return dict[key] if key in dict else default\n",
    "\n",
    "    def _init_qtable(self, state_space,action_space):\n",
    "        qtable = np.zeros((state_space, action_space))\n",
    "        return qtable\n",
    "    \n",
    "    def _epsilon_greedy_policy(self, state, epsilon):\n",
    "        random_init = random.uniform(-1,1)\n",
    "        if random_init > epsilon:\n",
    "            action = np.argmax(self.qtable[state])\n",
    "        else:\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        loop = tqdm(list(range(self.n_training_eps)))\n",
    "        #reset qtable\n",
    "        self._reset_qtable()\n",
    "\n",
    "        print(\"=\" * 20)\n",
    "        print(\"TRAINING\")\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "        for ep in loop:\n",
    "            epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_rate * ep)\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            for step in range(self.max_steps):\n",
    "                action = self._epsilon_greedy_policy(state, epsilon)\n",
    "                new_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                self.qtable[state][action] = self.qtable[state][action] + \\\n",
    "                    self.learning_rate * (reward  + self.gamma * np.max(self.qtable[new_state]) - self.qtable[state][action])\n",
    "                \n",
    "                state = new_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            loop.set_description(f\"ep = {ep}, eposilon = {epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Q-TABLE, shape = (16, 4)\n",
      "====================\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "====================\n",
      "PARAMS\n",
      "====================\n",
      "n_training_eps = 10000\n",
      "n_eval_eps = 100\n",
      "max_steps = 99\n",
      "learning_rate = 0.001\n",
      "max_epsilon = 1.0\n",
      "min_epsilon = 0.005\n",
      "decay rate = 0.0005\n",
      "gamma = 0.95\n",
      "====================\n",
      "ENV MAP\n",
      "====================\n",
      "['S', 'F', 'F', 'F']\n",
      "['F', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "====================\n",
      "ENV STATE\n",
      "====================\n",
      "[[1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "ENV DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 0.5}\n",
      "timestep = 0, max_timestep = 10,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [0, 0]\n",
      "goal position = [3, 3]\n",
      "terminated = False\n",
      "env = \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\",is_slippery=False)\n",
    "reward_dict = {\n",
    "    'G': 0.5,\n",
    "}\n",
    "env = GridWorldEnv(map = GET_MAP('4x4'), reward_dict = reward_dict, max_timestep = 10)\n",
    "\n",
    "qlearning = QLearningAlgorithm(env)\n",
    "print(qlearning)\n",
    "\n",
    "# qlearning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Q-TABLE, shape = (16, 4)\n",
      "====================\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "====================\n",
      "PARAMS\n",
      "====================\n",
      "n_training_eps = 10000\n",
      "n_eval_eps = 100\n",
      "max_steps = 99\n",
      "learning_rate = 0.001\n",
      "max_epsilon = 1.0\n",
      "min_epsilon = 0.005\n",
      "decay rate = 0.0005\n",
      "gamma = 0.95\n",
      "====================\n",
      "ENV MAP\n",
      "====================\n",
      "['S', 'F', 'F', 'F']\n",
      "['F', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "====================\n",
      "ENV STATE\n",
      "====================\n",
      "[[1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "ENV DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 0.5}\n",
      "timestep = 0, max_timestep = 10,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [0, 0]\n",
      "goal position = [3, 3]\n",
      "terminated = False\n",
      "env = \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(a[(0,0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test GridWorldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "ENV MAP\n",
      "====================\n",
      "['S', 'F', 'F', 'F']\n",
      "['F', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "====================\n",
      "ENV STATE\n",
      "====================\n",
      "[[1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "ENV DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 1}\n",
      "timestep = 0, max_timestep = 10,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [0, 0]\n",
      "goal position = [3, 3]\n",
      "terminated = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reward_dict = {\n",
    "    'G': 1,\n",
    "}\n",
    "env = GridWorldEnv(map = GET_MAP('4x4'), reward_dict = reward_dict, max_timestep = 10)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "[[0.  1.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "[[0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "[[0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "[[0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "exit\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "gridworld = GridWorldRenderer(4,4)\n",
    "\n",
    "step = [2,2,2,2,0,0,0,0,3,3,3,3,1,1,1,1]\n",
    "env.reset()\n",
    "print(env.state)\n",
    "print(\"=\" * 20)\n",
    "gridworld.start(env.state)\n",
    "i = 0\n",
    "iter = 0\n",
    "clock = 0\n",
    "update_time = 0\n",
    "\n",
    "while True:\n",
    "    running_time = int(pygame.time.get_ticks()/1000)\n",
    "    gridworld.update(env.state)\n",
    "    if running_time % 1 == 0 and update_time != running_time:\n",
    "        update_time = running_time\n",
    "        action = step[i]\n",
    "        i = (i + 1) % len(step)\n",
    "        env.step(action)\n",
    "        print(env.state)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render the agent movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
