{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame\n",
    "import sys\n",
    "\n",
    "colors = {\n",
    "    \"black\": (0,0,0),\n",
    "    \"white\": (255,255,255),\n",
    "    \"light_white\": (200,200,200),\n",
    "    \"blue\": (0,0,255),\n",
    "    \"green\": (51, 204, 51),\n",
    "}\n",
    "class GridWorldRenderer():\n",
    "    def __init__(self, rows, columns, cell_size = 50):\n",
    "        self.rows = rows \n",
    "        self.columns = columns\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        pygame.init()\n",
    "        self.test = False \n",
    "        self.test_image = False \n",
    "\n",
    "        self._window_width = self.columns * self.cell_size\n",
    "        self._window_height = self.rows * self.cell_size\n",
    "\n",
    "        self.screen = pygame.display.set_mode((self._window_width, self._window_height))\n",
    "        pygame.display.set_caption(f\"Grid World {self.rows}x{self.columns}\" )\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.border_color = colors['black']\n",
    "\n",
    "        self.update()\n",
    "\n",
    "    def _drawgrid(self):\n",
    "        for i, x in enumerate(range(0, self._window_width, self.cell_size)):\n",
    "            for j, y in enumerate(range(0, self._window_height, self.cell_size)):\n",
    "                color = colors['white']\n",
    "\n",
    "                rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "\n",
    "                pygame.draw.rect(self.screen, color, rect)\n",
    "                if self.test_image and i == 0 and j == 0 :\n",
    "                    self.draw_robot(self.screen, x,y)\n",
    "                if i == 1 and j == 0 :\n",
    "                    self.draw_battery(self.screen, x,y)\n",
    "                if i == 0 and j == 1 :\n",
    "                    self.draw_crap(self.screen, x,y)\n",
    "\n",
    "                border = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "                pygame.draw.rect(self.screen, self.border_color, border, 1)\n",
    "                \n",
    "    def toggle(self):\n",
    "        self.test = not self.test\n",
    "    \n",
    "    def _draw_object(self, screen, img, x,y):\n",
    "        img = pygame.image.load(img)\n",
    "        img = pygame.transform.scale(img, (self.cell_size, self.cell_size))\n",
    "        screen.blit(img, (x,y))\n",
    "        border = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, self.border_color, border, 1)\n",
    "\n",
    "    def draw_robot(self, screen, x, y):\n",
    "        robot_img = './assets/robot.jpg'\n",
    "        self._draw_object(screen, robot_img, x, y)\n",
    "\n",
    "    def draw_crap(self, screen, x, y):\n",
    "        img = './assets/crap.png'\n",
    "        self._draw_object(screen, img, x, y)\n",
    "\n",
    "    def draw_battery(self, screen, x, y):\n",
    "        img = './assets/battery.png'\n",
    "        self._draw_object(screen, img, x, y)\n",
    "    \n",
    "    def update(self):\n",
    "        # while True:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                print('exit')\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "            \n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                self.test_image = not self.test_image\n",
    "\n",
    "            self._drawgrid()\n",
    "            pygame.display.update()\n",
    "    \n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "gridworld = GridWorldRenderer(8,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    gridworld.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = [-1, 0]\n",
    "\n",
    "a = np.clip(a, 0, 2)\n",
    "print(a)\n",
    "\n",
    "print(np.all((a >=0) & (a <= 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_map():\n",
    "    return 'SFFH\\nH'\n",
    "\n",
    "def GET_MAP(map):\n",
    "    if map == \"4x4\": \n",
    "        return [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ]\n",
    "\n",
    "    if map == \"8x8\": \n",
    "        return [\n",
    "            \"SFFFFFFF\",\n",
    "            \"FFFFFFFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FFFFFHFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FHHFFFHF\",\n",
    "            \"FHFFHFHF\",\n",
    "            \"FFFHFFFG\",\n",
    "        ],\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self, map, reward_dict, **kwargs):\n",
    "\n",
    "        #Check if map has the same dimension as rows and cols\n",
    "        rows, cols, map = self._process_map(map)\n",
    "\n",
    "        self.map = map\n",
    "        nS = rows * cols\n",
    "        nA = 4\n",
    "        self.rows, self.cols = rows, cols\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [rows - 1, cols - 1]\n",
    "\n",
    "        #save reward dict\n",
    "        self.reward_dict = reward_dict\n",
    "\n",
    "        #default state\n",
    "        self.state = np.zeros((self.rows, self.cols))\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "\n",
    "        self.observation_space = spaces.Discrete(nS)\n",
    "        self.action_space = spaces.Discrete(nA)\n",
    "\n",
    "        #time step used to track how long this agent performing\n",
    "        #if it's to long, terminate early\n",
    "        self.timestep = 0\n",
    "        self.max_timestep = int(kwargs['max_timestep']) if kwargs['max_timestep'] != None else 100\n",
    "\n",
    "        #termination status\n",
    "        self.terminated = False\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def _process_map(self, map_data):\n",
    "\n",
    "        rows = map_data\n",
    "        rows_n = len(rows)\n",
    "        cols_n = len(rows[0])\n",
    "        map = [['' for _ in range(cols_n)] for _ in range(rows_n)]\n",
    "        for i, row in enumerate(rows):\n",
    "            for j, val in enumerate(row):\n",
    "                map[i][j] = val \n",
    "\n",
    "        return rows_n, cols_n, map\n",
    "\n",
    "    def set_state(self, agent_pos, goal_pos):\n",
    "        self.state = np.zeros((self.rows, self.cols))\n",
    "        self.state[tuple(agent_pos)] = 1\n",
    "        self.state[tuple(goal_pos)] = 0.5\n",
    "        observation = self.state.flatten()\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    #Step function: agent take step in env\n",
    "    def step(self, action):\n",
    "        #actions:\n",
    "        #0: down\n",
    "        #1:up \n",
    "        #2:right\n",
    "        #3:left\n",
    "        if self.terminated:\n",
    "            return\n",
    "\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] += 1\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] -= 1\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] += 1\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] -= 1\n",
    "\n",
    "        #clip the agent position to avoid out of bound\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.rows - 1)\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "        observation = self.state.flatten()\n",
    "\n",
    "        #Check if the agent takes too long to go to goal\n",
    "        self.timestep += 1\n",
    "        self.terminated = True if self.timestep > self.max_timestep else False\n",
    "\n",
    "        #Define your reward function\n",
    "        reward = 0\n",
    "\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
    "            self.terminated = True\n",
    "            reward = 1\n",
    "        \n",
    "        info = {}\n",
    "        return observation, reward, self.terminated, info\n",
    "    \n",
    "    def render(self):\n",
    "        #Put a renderer here\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        print(\"=\" * 20)\n",
    "        print('MAP')\n",
    "        print(\"=\" * 20)\n",
    "        for row in self.map:\n",
    "            print(row)\n",
    "        print(\"=\" * 20)\n",
    "        print('STATE')\n",
    "        print(\"=\" * 20)\n",
    "        print(self.state)\n",
    "        print(\"=\" * 20)\n",
    "        print(\"DESCRIPTION\")\n",
    "        print(\"=\" * 20)\n",
    "        print(f'reward dict = {self.reward_dict}')\n",
    "        print(f'timestep = {self.timestep}, max_timestep = {self.max_timestep},')\n",
    "        print(f'obs space = {self.observation_space.n}')\n",
    "        print(f'actions = {self.action_space.n}')\n",
    "        print(f'agent position = {self.agent_pos}')\n",
    "        print(f'goal position = {self.goal_pos}')\n",
    "        print(f'terminated = {self.terminated}')\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAlgorithm():\n",
    "    def __init__(self, env, **kwargs: dict) -> None:\n",
    "        self.n_training_eps = int(self._get(kwargs, \"n_training_eps\", 10000))\n",
    "        self.n_eval_eps = int(self._get(kwargs, \"n_eval_eps\", 100))\n",
    "        self.max_steps = int(self._get(kwargs, \"max_steps\", 99))\n",
    "        self.learning_rate = float(self._get(kwargs, \"learning_rate\", 0.001))\n",
    "        self.max_epsilon = float(self._get(kwargs, \"max_epsilon\", 1.0))\n",
    "        self.min_epsilon = float(self._get(kwargs, \"min_epsilon\", 0.005))\n",
    "        self.decay_rate = float(self._get(kwargs, \"decay_rate\", 0.005))\n",
    "        self.gamma = float(self._get(kwargs, \"gamma\", 0.95))\n",
    "        self.qtable = self._init_qtable(env.observation_space.n, env.action_space.n)\n",
    "        self.env = env\n",
    "\n",
    "    def _get(self, dict, key, default):\n",
    "        return dict[key] if key in dict else default\n",
    "\n",
    "    def _init_qtable(self, state_space,action_space):\n",
    "        qtable = np.zeros((state_space, action_space))\n",
    "        return qtable\n",
    "    \n",
    "    def _epsilon_greedy_policy(self, state, epsilon):\n",
    "        random_init = random.uniform(-1,1)\n",
    "        if random_init > epsilon:\n",
    "            action = np.argmax(self.qtable[state])\n",
    "        else:\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        loop = tqdm(list(range(self.n_training_eps)))\n",
    "        print('qtable shape =', self.qtable.shape)\n",
    "\n",
    "        for ep in loop:\n",
    "            epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_rate * ep)\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            for step in range(self.max_steps):\n",
    "                action = self._epsilon_greedy_policy(state, epsilon)\n",
    "                new_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                self.qtable[state][action] = self.qtable[state][action] + \\\n",
    "                    self.learning_rate * (reward  + self.gamma * np.max(self.qtable[new_state]) - self.qtable[state][action])\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = new_state\n",
    "            \n",
    "            \n",
    "            loop.set_description(f\"ep = {ep}, eposilon = {epsilon:.2f}\")\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        print(self.qtable)\n",
    "        return ''\n",
    "\n",
    "\n",
    "# class RLController():\n",
    "#     def __init__(self, **kwargs) -> None:\n",
    "\n",
    "#         self.n_training_eps = int(self._get(kwargs, \"n_training_eps\", 10000))\n",
    "#         self.n_eval_eps = int(self._get(kwargs, \"n_eval_eps\", 100))\n",
    "#         self.learning_rate = float(self._get(kwargs, \"learning_rate\", 0.001))\n",
    "#         self.max_epsilon = float(self._get(kwargs, \"max_epsilon\", 1.0))\n",
    "#         self.min_epsilon = float(self._get(kwargs, \"min_epsilon\", 0.005))\n",
    "#         self.decay_rate = float(self._get(kwargs, \"decay_rate\", 0.005))\n",
    "#         # self.env = GridWorldEnv()\n",
    "#         reward_dict = {\n",
    "#             'G': 1\n",
    "#         }\n",
    "#         self.env = GridWorldEnv(map = GET_MAP('4x4'), reward_dict = reward_dict, max_timestep = 10)\n",
    "\n",
    "#         obs_space = self.env.observation_space\n",
    "#         action_space = self.env.action_space\n",
    "#         self.qlearning = QLearningLearning(obs_space, action_space)\n",
    "    \n",
    "#     def _get(self, dict, key, default):\n",
    "#         return dict[key] if key in dict else default\n",
    "            \n",
    "#     def train(self):\n",
    "#         loop = tqdm(list(range(self.n_training_eps)))\n",
    "#         print('qtable shape =', qtable.shape)\n",
    "\n",
    "#         for ep in loop:\n",
    "#             epsilon = self.min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * ep)\n",
    "#             state = env.reset()\n",
    "#             done = False\n",
    "\n",
    "#             for step in range(max_steps):\n",
    "#                 action = epsilon_greedy_policy(qtable, env, state,epsilon)\n",
    "#                 new_state, reward, done, info = env.step(action)\n",
    "\n",
    "#                 qtable[state][action] = qtable[state][action] + \\\n",
    "#                     learning_rate * (reward  + gamma * np.max(qtable[new_state]) - qtable[state][action])\n",
    "                \n",
    "#                 if done:\n",
    "#                     break\n",
    "                \n",
    "#                 state = new_state\n",
    "            \n",
    "            \n",
    "#             loop.set_description(f\"ep = {ep}, eposilon = {epsilon:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtable shape = (16, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QLearningAlgorithm' object has no attribute 'gamma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\ML\\ml_projects\\rl\\qlearning_gridworld.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m qlearning \u001b[39m=\u001b[39m QLearningAlgorithm(env)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(qlearning\u001b[39m.\u001b[39mqtable\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m qlearning\u001b[39m.\u001b[39;49mtrain()\n",
      "\u001b[1;32me:\\ML\\ml_projects\\rl\\qlearning_gridworld.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X24sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epsilon_greedy_policy(state, epsilon)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X24sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m new_state, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X24sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqtable[state][action] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqtable[state][action] \u001b[39m+\u001b[39m \\\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X24sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m*\u001b[39m (reward  \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqtable[new_state]) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqtable[state][action])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X24sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X24sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'QLearningAlgorithm' object has no attribute 'gamma'"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\",is_slippery=False)\n",
    "\n",
    "qlearning = QLearningAlgorithm(env)\n",
    "print(qlearning.qtable.shape)\n",
    "\n",
    "qlearning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test GridWorldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "MAP\n",
      "====================\n",
      "['S', 'F', 'F', 'F']\n",
      "['F', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "====================\n",
      "STATE\n",
      "====================\n",
      "[[1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 1}\n",
      "timestep = 0, max_timestep = 10,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [0, 0]\n",
      "goal position = [3, 3]\n",
      "terminated = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reward_dict = {\n",
    "    'G': 1,\n",
    "}\n",
    "env = GridWorldEnv(map = GET_MAP('4x4'), reward_dict = reward_dict, max_timestep = 10)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "MAP\n",
      "====================\n",
      "['S', 'F', 'F', 'F']\n",
      "['F', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "====================\n",
      "STATE\n",
      "====================\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 1}\n",
      "timestep = 6, max_timestep = 10,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [3 3]\n",
      "goal position = [3, 3]\n",
      "terminated = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.step(0)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
