{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame\n",
    "import sys\n",
    "import time\n",
    "\n",
    "colors = {\n",
    "    \"black\": (0,0,0),\n",
    "    \"white\": (255,255,255),\n",
    "    \"light_white\": (200,200,200),\n",
    "    \"blue\": (0,0,255),\n",
    "    \"green\": (51, 204, 51),\n",
    "}\n",
    "class GridWorldRenderer():\n",
    "    def __init__(self, rows, columns, cell_size = 50):\n",
    "        self.rows = rows \n",
    "        self.columns = columns\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "    def start(self, state):\n",
    "        pygame.init()\n",
    "        # self.test = False \n",
    "        # self.test_image = False \n",
    "\n",
    "        self._window_width = self.columns * self.cell_size\n",
    "        self._window_height = self.rows * self.cell_size\n",
    "\n",
    "        self.screen = pygame.display.set_mode((self._window_width, self._window_height))\n",
    "        pygame.display.set_caption(f\"Grid World {self.rows}x{self.columns}\" )\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.screen.fill(colors['white'])\n",
    "\n",
    "        self.border_color = colors['black']\n",
    "        self.state = state\n",
    "\n",
    "        self.update(state)\n",
    "\n",
    "    # def _drawgrid(self):\n",
    "    #     for i, x in enumerate(range(0, self._window_width, self.cell_size)):\n",
    "    #         for j, y in enumerate(range(0, self._window_height, self.cell_size)):\n",
    "    #             color = colors['white']\n",
    "\n",
    "    #             rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "\n",
    "    #             pygame.draw.rect(self.screen, color, rect)\n",
    "    #             if self.test_image and i == 0 and j == 0 :\n",
    "    #                 self.draw_robot(self.screen, x,y)\n",
    "    #             if i == 1 and j == 0 :\n",
    "    #                 self.draw_battery(self.screen, x,y)\n",
    "    #             if i == 0 and j == 1 :\n",
    "    #                 self.draw_crap(self.screen, x,y)\n",
    "\n",
    "    #             border = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "    #             pygame.draw.rect(self.screen, self.border_color, border, 1)\n",
    "\n",
    "    def _drawstate(self):\n",
    "        for i, x in enumerate(range(0, self._window_width, self.cell_size)):\n",
    "            for j, y in enumerate(range(0, self._window_height, self.cell_size)):\n",
    "                if self.state[i][j] == 1:\n",
    "                    rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "                    blue = colors['blue']\n",
    "                    pygame.draw.rect(self.screen, blue, rect)\n",
    "                else:\n",
    "                    rect = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "                    white = colors['white']\n",
    "                    pygame.draw.rect(self.screen, white, rect)\n",
    "\n",
    "                black = colors['black']\n",
    "                pygame.draw.rect(self.screen, black, rect, 1)\n",
    "                \n",
    "\n",
    "    # def toggle(self):\n",
    "    #     self.test = not self.test\n",
    "    \n",
    "    # def _draw_object(self, screen, img, x,y):\n",
    "    #     img = pygame.image.load(img)\n",
    "    #     img = pygame.transform.scale(img, (self.cell_size, self.cell_size))\n",
    "    #     screen.blit(img, (x,y))\n",
    "    #     border = pygame.Rect(x,y, self.cell_size, self.cell_size)\n",
    "    #     pygame.draw.rect(self.screen, self.border_color, border, 1)\n",
    "\n",
    "    # def draw_robot(self, screen, x, y):\n",
    "    #     robot_img = './assets/robot.jpg'\n",
    "    #     self._draw_object(screen, robot_img, x, y)\n",
    "\n",
    "    # def draw_crap(self, screen, x, y):\n",
    "    #     img = './assets/crap.png'\n",
    "    #     self._draw_object(screen, img, x, y)\n",
    "\n",
    "    # def draw_battery(self, screen, x, y):\n",
    "    #     img = './assets/battery.png'\n",
    "    #     self._draw_object(screen, img, x, y)\n",
    "    \n",
    "    def update(self, new_state):\n",
    "        #clear screen\n",
    "        self.clock.tick(60)\n",
    "        self.screen.fill(colors['white'])\n",
    "        \n",
    "        self.state = new_state\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.end()\n",
    "            \n",
    "\n",
    "        self._drawstate()\n",
    "        pygame.display.update()\n",
    "        \n",
    "    def end(self):\n",
    "        print('exit')\n",
    "        pygame.quit()\n",
    "        sys.exit()\n",
    "\n",
    "    \n",
    "    #call a while  loop to update pygame drawing\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "# gridworld = GridWorldRenderer(8,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAlgorithm():\n",
    "    def __init__(self, env, **kwargs: dict) -> None:\n",
    "        self.n_training_eps = int(self._get(kwargs, \"n_training_eps\", 10000))\n",
    "        self.n_eval_eps = int(self._get(kwargs, \"n_eval_eps\", 100))\n",
    "        self.max_steps = int(self._get(kwargs, \"max_steps\", 99))\n",
    "        self.learning_rate = float(self._get(kwargs, \"learning_rate\", 0.001))\n",
    "        self.max_epsilon = float(self._get(kwargs, \"max_epsilon\", 1.0))\n",
    "        self.min_epsilon = float(self._get(kwargs, \"min_epsilon\", 0.005))\n",
    "        self.decay_rate = float(self._get(kwargs, \"decay_rate\", 0.0005))\n",
    "        self.gamma = float(self._get(kwargs, \"gamma\", 0.95))\n",
    "        self.env = env\n",
    "        self._reset_qtable()\n",
    "    \n",
    "    def _reset_qtable(self):\n",
    "        self.qtable = self._init_qtable(self.env.observation_space.n, self.env.action_space.n)\n",
    "\n",
    "    def __str__(self):\n",
    "        print(\"=\" * 20)\n",
    "        print(f\"Q-TABLE, shape = {self.qtable.shape}\")\n",
    "        print(\"=\" * 20)\n",
    "        print(self.qtable)\n",
    "        print(\"=\" * 20)\n",
    "        print(\"PARAMS\")\n",
    "        print(\"=\" * 20)\n",
    "        print(f'n_training_eps = {self.n_training_eps}')\n",
    "        print(f'n_eval_eps = {self.n_eval_eps}')\n",
    "        print(f'max_steps = {self.max_steps}')\n",
    "        print(f'learning_rate = {self.learning_rate}')\n",
    "        print(f'max_epsilon = {self.max_epsilon}')\n",
    "        print(f'min_epsilon = {self.min_epsilon}')\n",
    "        print(f'decay rate = {self.decay_rate}')\n",
    "        print(f'gamma = {self.gamma}')\n",
    "        print(f'env = {self.env}')\n",
    "        return ''\n",
    "\n",
    "    def _get(self, dict, key, default):\n",
    "        return dict[key] if key in dict else default\n",
    "\n",
    "    def _init_qtable(self, state_space,action_space):\n",
    "        qtable = np.zeros((state_space, action_space))\n",
    "        return qtable\n",
    "    \n",
    "    def _epsilon_greedy_policy(self, state, epsilon):\n",
    "        random_init = random.uniform(-1,1)\n",
    "        if random_init > epsilon:\n",
    "            action = np.argmax(self.qtable[state])\n",
    "        else:\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        loop = tqdm(list(range(self.n_training_eps)))\n",
    "        #reset qtable\n",
    "        self._reset_qtable()\n",
    "\n",
    "        print(\"=\" * 20)\n",
    "        print(\"TRAINING\")\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "        for ep in loop:\n",
    "            epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_rate * ep)\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            for step in range(self.max_steps):\n",
    "                action = self._epsilon_greedy_policy(state, epsilon)\n",
    "                new_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                self.qtable[state][action] = self.qtable[state][action] + \\\n",
    "                    self.learning_rate * (reward  + self.gamma * np.max(self.qtable[new_state]) - self.qtable[state][action])\n",
    "                \n",
    "                state = new_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            loop.set_description(f\"ep = {ep}, eposilon = {epsilon:.2f}\")\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return np.argmax(self.qtable[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_map():\n",
    "    return 'SFFH\\nH'\n",
    "\n",
    "def GET_MAP(map):\n",
    "    if map == \"4x4\": \n",
    "        return [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ]\n",
    "\n",
    "    if map == \"8x8\": \n",
    "        return [\n",
    "            \"SFFFFFFF\",\n",
    "            \"FFFFFFFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FFFFFHFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FHHFFFHF\",\n",
    "            \"FHFFHFHF\",\n",
    "            \"FFFHFFFG\",\n",
    "        ],\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self, map, reward_dict, **kwargs):\n",
    "\n",
    "        #Check if map has the same dimension as rows and cols\n",
    "        rows, cols, map = self._process_map(map)\n",
    "\n",
    "        self.map = map\n",
    "        nS = rows * cols\n",
    "        nA = 4\n",
    "        self.rows, self.cols = rows, cols\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [rows - 1, cols - 1]\n",
    "\n",
    "        #save reward dict\n",
    "        self.reward_dict = reward_dict\n",
    "\n",
    "        #default state\n",
    "        self.state = np.zeros((self.rows, self.cols))\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "\n",
    "        self.observation_space = spaces.Discrete(nS)\n",
    "        self.action_space = spaces.Discrete(nA)\n",
    "\n",
    "        #time step used to track how long this agent performing\n",
    "        #if it's to long, terminate early\n",
    "        self.timestep = 0\n",
    "        self.max_timestep = int(kwargs['max_timestep']) if kwargs['max_timestep'] != None else 100\n",
    "\n",
    "        #termination status\n",
    "        self.terminated = False\n",
    "\n",
    "        #render using pygame\n",
    "        self.renderer = GridWorldRenderer(rows, cols)\n",
    "    \n",
    "    #Reset environmentj\n",
    "    def reset(self):\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [self.rows - 1, self.cols - 1]\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "        self.timestep = 0\n",
    "        return 0\n",
    "\n",
    "    def _process_map(self, map_data):\n",
    "        rows = map_data\n",
    "        rows_n = len(rows)\n",
    "        cols_n = len(rows[0])\n",
    "        map = [['' for _ in range(cols_n)] for _ in range(rows_n)]\n",
    "        for i, row in enumerate(rows):\n",
    "            for j, val in enumerate(row):\n",
    "                map[i][j] = val \n",
    "\n",
    "        return rows_n, cols_n, map\n",
    "\n",
    "    def set_state(self, agent_pos, goal_pos):\n",
    "        self.state = np.zeros((self.rows, self.cols))\n",
    "        self.state[tuple(agent_pos)] = 1\n",
    "        self.state[tuple(goal_pos)] = 0.5\n",
    "        observation = self.state.flatten()\n",
    "        return observation\n",
    "    \n",
    "    def reached_goal(self, pos):\n",
    "        x,y = pos\n",
    "        return True if self.map[x][y] == 'G'else False\n",
    "    \n",
    "    def get_reward(self, pos):\n",
    "        x,y = pos\n",
    "        val = self.map[x][y]\n",
    "        return self.reward_dict[val] if val in self.reward_dict else 0\n",
    "\n",
    "    #Step function: agent take step in env\n",
    "    def step(self, action):\n",
    "        #actions:\n",
    "        #0: down\n",
    "        #1:up \n",
    "        #2:right\n",
    "        #3:left\n",
    "\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] += 1\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] -= 1\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] += 1\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] -= 1\n",
    "\n",
    "        #clip the agent position to avoid out of bound\n",
    "        self.agent_pos[0] = np.clip(self.agent_pos[0], 0, self.cols - 1)\n",
    "        self.agent_pos[1] = np.clip(self.agent_pos[1], 0, self.rows - 1)\n",
    "\n",
    "        self.set_state(self.agent_pos, self.goal_pos)\n",
    "        observation = self.state.flatten()\n",
    "\n",
    "        #Check if the agent takes too long to go to goal\n",
    "        self.timestep += 1\n",
    "        self.terminated = True if self.timestep > self.max_timestep else False\n",
    "\n",
    "        #Define your reward function\n",
    "        reward = self.get_reward(self.agent_pos)\n",
    "\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
    "            self.terminated = True\n",
    "            reward = 1\n",
    "        \n",
    "        if self.reached_goal(self.agent_pos):\n",
    "            self.terminated = True\n",
    "        \n",
    "        info = {}\n",
    "        #return:\n",
    "        #next state, argmax to get the new state of agent, np.argmax([0,0,1,0,0,0,0.5]) = 2\n",
    "        #reward\n",
    "        #done or not\n",
    "        #extra infomation\n",
    "        return np.argmax(observation), reward, self.terminated, info\n",
    "    \n",
    "    def render(self, agent: QLearningAlgorithm) -> None:\n",
    "        #Put a renderer here\n",
    "        self.reset()\n",
    "        self.renderer.start(self.state)\n",
    "        max_iter = 100\n",
    "        iter =0 \n",
    "        curr_state = 0\n",
    "        while self.terminated == False and iter < max_iter:\n",
    "            self.renderer.update(self.state)\n",
    "            observation = self.state.flatten()\n",
    "            curr_state = np.argmax(observation)\n",
    "            action = agent.get_action(curr_state)\n",
    "\n",
    "            curr_state, _,_,_ = self.step(action)\n",
    "            pygame.time.wait(500)\n",
    "            iter += 1\n",
    "        self.renderer.end()\n",
    "        print(\"Terminated\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        print(\"=\" * 20)\n",
    "        print('ENV MAP')\n",
    "        print(\"=\" * 20)\n",
    "        for row in self.map:\n",
    "            print(row)\n",
    "        print(\"=\" * 20)\n",
    "        print('ENV STATE')\n",
    "        print(\"=\" * 20)\n",
    "        print(self.state)\n",
    "        print(\"=\" * 20)\n",
    "        print(\"ENV DESCRIPTION\")\n",
    "        print(\"=\" * 20)\n",
    "        print(f'reward dict = {self.reward_dict}')\n",
    "        print(f'timestep = {self.timestep}, max_timestep = {self.max_timestep},')\n",
    "        print(f'obs space = {self.observation_space.n}')\n",
    "        print(f'actions = {self.action_space.n}')\n",
    "        print(f'agent position = {self.agent_pos}')\n",
    "        print(f'goal position = {self.goal_pos}')\n",
    "        print(f'terminated = {self.terminated}')\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent import QLearningAgent\n",
    "from src.env import GridWorldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\ML\\ml_projects\\rl\\src\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\",is_slippery=False)\n",
    "reward_dict = {\n",
    "    'G': 1,\n",
    "}\n",
    "env = GridWorldEnv('4x4', reward_dict = reward_dict, max_timestep = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep = 66, eposilon = 0.97:   1%|          | 60/10000 [00:00<00:16, 594.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "TRAINING\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep = 4852, eposilon = 0.09:  48%|████▊     | 4822/10000 [00:07<00:07, 648.69it/s]"
     ]
    }
   ],
   "source": [
    "qlearning = QLearningAgent(env)\n",
    "# print(qlearning)\n",
    "qlearning.train()\n",
    "qlearning.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = GridWorldEnv('4x4', reward_dict = reward_dict, max_timestep = 10)\n",
    "env.render(qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Q-TABLE, shape = (16, 4)\n",
      "====================\n",
      "[[0.09949599 0.11322295 0.36305579 0.12115583]\n",
      " [0.47276961 0.15376244 0.0712842  0.10412594]\n",
      " [0.21747489 0.01687942 0.0089549  0.04121438]\n",
      " [0.08598744 0.00277095 0.00310443 0.00603573]\n",
      " [0.03180685 0.04334721 0.30378327 0.03944882]\n",
      " [0.60242254 0.14529019 0.22189815 0.09074389]\n",
      " [0.57151982 0.02734006 0.10009132 0.09252812]\n",
      " [0.50228571 0.00580963 0.04459946 0.05191821]\n",
      " [0.00381836 0.01686956 0.26355942 0.01684962]\n",
      " [0.1245455  0.19352862 0.73239184 0.07149765]\n",
      " [0.35437275 0.17116539 0.86928223 0.25285425]\n",
      " [0.97256123 0.14824445 0.40984399 0.2756309 ]\n",
      " [0.00123246 0.00506839 0.05341668 0.00131691]\n",
      " [0.03208811 0.06096813 0.36581754 0.00425051]\n",
      " [0.12264679 0.12525629 0.76723571 0.03907166]\n",
      " [0.         0.         0.         0.        ]]\n",
      "====================\n",
      "PARAMS\n",
      "====================\n",
      "n_training_eps = 10000\n",
      "n_eval_eps = 100\n",
      "max_steps = 99\n",
      "learning_rate = 0.001\n",
      "max_epsilon = 1.0\n",
      "min_epsilon = 0.005\n",
      "decay rate = 0.0005\n",
      "gamma = 0.95\n",
      "====================\n",
      "ENV MAP\n",
      "====================\n",
      "['S', 'F', 'F', 'F']\n",
      "['F', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "====================\n",
      "ENV STATE\n",
      "====================\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "ENV DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 1}\n",
      "timestep = 6, max_timestep = 10,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [3, 3]\n",
      "goal position = [3, 3]\n",
      "terminated = True\n",
      "env = \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearning.qtable\n",
    "#save qtable\n",
    "with open('qtable.npy', 'wb') as file:\n",
    "    np.save(file, qlearning.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.95689949e-01 1.38064714e-01 1.06121211e-01 1.35516080e-01]\n",
      " [3.18065017e-01 3.82771463e-02 6.15608108e-03 4.34399335e-02]\n",
      " [5.39815032e-02 1.68079049e-03 2.32974938e-04 9.53439141e-03]\n",
      " [5.84049610e-03 6.89223099e-05 6.28668322e-05 3.17291475e-04]\n",
      " [1.44302505e-01 1.20382128e-01 5.09127587e-01 1.66794701e-01]\n",
      " [6.42008184e-01 9.20341031e-02 1.17422646e-01 1.66434772e-01]\n",
      " [3.29896150e-01 4.05492912e-03 1.06120522e-02 6.45226069e-02]\n",
      " [8.74166558e-02 1.67897546e-04 3.32167255e-03 7.29796541e-03]\n",
      " [5.84698450e-02 5.28496164e-02 4.20337201e-01 4.89881527e-02]\n",
      " [7.74774138e-01 2.30768245e-01 2.95454158e-01 1.24687229e-01]\n",
      " [7.21301622e-01 4.26779797e-02 1.00492352e-01 1.30932674e-01]\n",
      " [4.38019536e-01 3.81403863e-03 2.98114722e-02 3.52019758e-02]\n",
      " [2.77711916e-02 2.98212636e-02 3.99026040e-01 3.47768298e-02]\n",
      " [3.79940201e-01 2.80023416e-01 8.91075089e-01 1.22651781e-01]\n",
      " [4.78308328e-01 2.60349632e-01 9.84378075e-01 3.37625110e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#load qtable\n",
    "with open('qtable.npy', 'rb') as file:\n",
    "    qtable = np.load(file)\n",
    "\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(a[(0,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "ENV MAP\n",
      "====================\n",
      "['S', 'F', 'F', 'F']\n",
      "['F', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "====================\n",
      "ENV STATE\n",
      "====================\n",
      "[[1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "ENV DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 1}\n",
      "timestep = 0, max_timestep = 10,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [0, 0]\n",
      "goal position = [3, 3]\n",
      "terminated = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reward_dict = {\n",
    "    'G': 1,\n",
    "}\n",
    "env = GridWorldEnv(map = GET_MAP('4x4'), reward_dict = reward_dict, max_timestep = 10)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "env.render(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Renderere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.env import GridWorldRenderer, GridWorldEnv\n",
    "from src.agent import QLearningAgent\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A--B\n",
      "--BR\n",
      "----\n",
      "B--G\n"
     ]
    }
   ],
   "source": [
    "map = [\n",
    "    ['A', 'R', '', 'S'],\n",
    "    ['', '', '', ''],\n",
    "    ['B', '', 'B', 'R'],\n",
    "    ['', '', '', 'G']\n",
    "    ]\n",
    "\n",
    "rows = 4 \n",
    "cols =4 \n",
    "\n",
    "def gen_obj():\n",
    "    choices = ['R', 'S', 'B', '-']\n",
    "    weights =  [0.05, 0.05, 0.05, 0.85]\n",
    "    return np.random.choice(choices, p = weights)\n",
    "\n",
    "map = [[gen_obj() for _ in range(rows)] for _ in range(cols)]\n",
    "map[0][0] = 'A'\n",
    "map[rows - 1][cols - 1] = 'G'\n",
    "\n",
    "map = [''.join(row) for row in map]\n",
    "\n",
    "for row in map:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.94331337e+01  1.73022398e+01  1.71475241e+01  1.73079067e+01]\n",
      " [ 1.54318993e+01  1.30066867e+01  8.40089874e+00  1.83807183e+01]\n",
      " [ 4.28382953e+00  1.99359493e+00  1.77101893e-01  1.40452154e+01]\n",
      " [ 1.21162336e+00 -3.89155858e-01 -4.06209135e-01  8.71076482e-01]\n",
      " [ 2.04662011e+01  1.84452127e+01  1.84367522e+01  1.84287353e+01]\n",
      " [ 1.77511937e+01  1.66063534e+01  1.50838763e+01  1.94263437e+01]\n",
      " [ 6.28635991e+00  5.14440615e+00  1.79720533e+00  1.76086234e+01]\n",
      " [ 1.28879245e+00  9.79087665e-02  3.74045197e-03  7.21647972e+00]\n",
      " [ 1.94146321e+01  1.94405378e+01  1.84075971e+01  1.74539225e+01]\n",
      " [ 1.71506018e+01  1.77314631e+01  1.40113240e+01  2.04503409e+01]\n",
      " [ 2.88491502e+00  6.07424896e+00  1.36734599e+00  1.70712707e+01]\n",
      " [ 2.49504139e+00  4.39674854e-01  1.66887851e-01  1.69518770e+00]\n",
      " [ 1.81152771e+01  2.04553918e+01  1.75986175e+01  1.81169056e+01]\n",
      " [ 1.17283095e+01  1.35548018e+01  4.24256163e+00  1.92961728e+01]\n",
      " [ 1.21005319e+00  3.17041091e+00  2.86586946e+00  9.63399243e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "reward_dict = {\n",
    "    'G': 3,\n",
    "    'B': 2,\n",
    "    'R': -1,\n",
    "    'S': -2,\n",
    "    'out-of-bound': -5,\n",
    "    'terminated': -3,\n",
    "}\n",
    "map = [\n",
    "    \"A---\",\n",
    "    \"----\",\n",
    "    \"BR--\",\n",
    "    \"---G\",\n",
    "]\n",
    "env = GridWorldEnv(map = map, reward_dict = reward_dict, max_timestep = 100)\n",
    "agent = QLearningAgent(env)\n",
    "# agent.train()\n",
    "# agent.save()\n",
    "agent.load('./qtable.npy')\n",
    "print(agent.qtable)\n",
    "\n",
    "# print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "['A', '-', '-', '-']\n",
      "['-', '-', '-', '-']\n",
      "['B', 'R', '-', '-']\n",
      "['-', '-', '-', 'G']\n",
      "(15, -1, True, {})\n"
     ]
    }
   ],
   "source": [
    "info = env.step(2)\n",
    "print(env.state)\n",
    "for row in env.map:\n",
    "    print(row)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "MAP\n",
      "['A', '-', '-', '-']\n",
      "['-', '-', '-', '-']\n",
      "['B', 'R', '-', '-']\n",
      "['-', '-', '-', 'G']\n",
      "====================\n",
      "took action =  0  to state  4\n",
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  0  to state  8\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  1  to state  4\n",
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  0  to state  8\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  1  to state  4\n",
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  0  to state  8\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  1  to state  4\n",
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  0  to state  8\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  1  to state  4\n",
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  0  to state  8\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  1  to state  4\n",
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  0  to state  8\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  1  to state  4\n",
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  0  to state  8\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  1  to state  4\n",
      "[[0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n",
      "took action =  0  to state  8\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5]]\n",
      "====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\ML\\ml_projects\\rl\\qlearning_gridworld.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender_simple(agent)\n",
      "File \u001b[1;32me:\\ML\\ml_projects\\rl\\src\\env.py:200\u001b[0m, in \u001b[0;36mGridWorldEnv.render_simple\u001b[1;34m(self, agent)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate)\n\u001b[0;32m    199\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m20\u001b[39m)\n\u001b[1;32m--> 200\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.5\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.render_simple(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Q-TABLE, shape = (16, 4)\n",
      "====================\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "====================\n",
      "PARAMS\n",
      "====================\n",
      "n_training_eps = 10000\n",
      "n_eval_eps = 100\n",
      "max_steps = 200\n",
      "learning_rate = 0.001\n",
      "max_epsilon = 1.0\n",
      "min_epsilon = 0.005\n",
      "decay rate = 0.0005\n",
      "gamma = 0.95\n",
      "====================\n",
      "ENV MAP\n",
      "====================\n",
      "['A', '-', '-', '-']\n",
      "['-', '-', '-', '-']\n",
      "['B', 'R', '-', '-']\n",
      "['-', '-', '-', 'G']\n",
      "====================\n",
      "ENV STATE\n",
      "====================\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  1.  0.  0.5]]\n",
      "====================\n",
      "ENV DESCRIPTION\n",
      "====================\n",
      "reward dict = {'G': 3, 'B': 2, 'R': -1, 'S': -2}\n",
      "timestep = 7, max_timestep = 100,\n",
      "obs space = 16\n",
      "actions = 4\n",
      "agent position = [4, 1]\n",
      "goal position = [3, 3]\n",
      "terminated = False\n",
      "env = \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "TRAINING\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\ML\\ml_projects\\rl\\qlearning_gridworld.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent \u001b[39m=\u001b[39m QLearningAgent(env, max_steps \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(agent)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m agent\u001b[39m.\u001b[39msave()\n",
      "File \u001b[1;32me:\\ML\\ml_projects\\rl\\src\\agent.py:75\u001b[0m, in \u001b[0;36mQLearningAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps):\n\u001b[0;32m     74\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epsilon_greedy_policy(state, epsilon)\n\u001b[1;32m---> 75\u001b[0m     new_state, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqtable[state][action] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqtable[state][action] \u001b[39m+\u001b[39m \\\n\u001b[0;32m     78\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m*\u001b[39m (reward  \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqtable[new_state]) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqtable[state][action])\n\u001b[0;32m     80\u001b[0m     state \u001b[39m=\u001b[39m new_state\n",
      "File \u001b[1;32me:\\ML\\ml_projects\\rl\\src\\env.py:149\u001b[0m, in \u001b[0;36mGridWorldEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_pos[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[39m#Define your reward function\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_reward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent_pos)\n\u001b[0;32m    151\u001b[0m \u001b[39m#clip the agent position to avoid out of bound\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_pos[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_pos[\u001b[39m0\u001b[39m], \u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcols \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32me:\\ML\\ml_projects\\rl\\src\\env.py:116\u001b[0m, in \u001b[0;36mGridWorldEnv.get_reward\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_reward\u001b[39m(\u001b[39mself\u001b[39m, pos):\n\u001b[0;32m    115\u001b[0m     x,y \u001b[39m=\u001b[39m pos\n\u001b[1;32m--> 116\u001b[0m     val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap[x][y]\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m x \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcols \u001b[39mor\u001b[39;00m y \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m y \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrows:\n\u001b[0;32m    118\u001b[0m         val \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "agent = QLearningAgent(env, max_steps = 200)\n",
    "print(agent)\n",
    "agent.train()\n",
    "agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37.99841867 35.59281339 32.77162579 35.54556581]\n",
      " [27.45043182 23.76353979 13.63400298 35.95221693]\n",
      " [ 8.62379783  5.2020622   0.83496694 26.37552016]\n",
      " [ 0.60434528  0.18024974  0.17498958  5.72524838]\n",
      " [39.99923927 36.0894567  36.08305595 37.99398737]\n",
      " [35.34697408 30.94700231 27.72537515 37.99700723]\n",
      " [16.01948031  8.40583918  3.22011847 34.41264039]\n",
      " [ 1.49783784  0.32244212  0.88319057 12.814537  ]\n",
      " [37.99902403 37.99904318 36.99887677 39.99935853]\n",
      " [36.00245775 36.00404187 34.59107063 39.9991033 ]\n",
      " [18.25235434 19.17633682  7.96456602 36.83781842]\n",
      " [ 0.72846166  1.06870185  1.80261941 19.18696926]\n",
      " [37.98994321 39.99921512 36.07392497 37.98991065]\n",
      " [34.33821483 35.36339968 26.9367389  37.99676534]\n",
      " [11.51062226 15.71171512  0.79947243 33.82815573]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(agent.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridWorldEnv' object has no attribute 'render_simple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\ML\\ml_projects\\rl\\qlearning_gridworld.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/rl/qlearning_gridworld.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender_simple(agent)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridWorldEnv' object has no attribute 'render_simple'"
     ]
    }
   ],
   "source": [
    "env.render_simple(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
