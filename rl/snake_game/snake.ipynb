{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.28.4, Python 3.11.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from snakegame.game import SnakeGameAI, Direction, Point\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearQnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def save(self, path='model.h5'):\n",
    "        torch.save(self.model, path)\n",
    "\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), self.lr)\n",
    "        # losss = (Q_new - Q)**2\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def train_step(self, state, action, reward, state_new, done):\n",
    "        state =     torch.tensor(state, dtype=torch.float)\n",
    "        state_new = torch.tensor(state_new, dtype=torch.float)\n",
    "        action =    torch.tensor(action, dtype=torch.long)\n",
    "        reward =    torch.tensor(reward, dtype=torch.float)\n",
    "\n",
    "        # (n,x)\n",
    "        # Why do we need this \n",
    "        if len(state.shape) == 1:\n",
    "            # (1,x)\n",
    "            state =         torch.unsqueeze(state, 0)\n",
    "            state_new =    torch.unsqueeze(state_new, 0)\n",
    "            action =        torch.unsqueeze(action, 0)\n",
    "            reward =        torch.unsqueeze(reward, 0)\n",
    "            done =          (done,)\n",
    "        \n",
    "        # 1: predicted Q valus with current state\n",
    "        pred = self.model(state)\n",
    "\n",
    "        # 2: Q_new = r + discounted_factor * max(next_pred Q value) \n",
    "        # pred.clone()\n",
    "        # preds[argmax(action)] = Q_new\n",
    "        # only do this if not done\n",
    "        target = pred.clone()\n",
    "        for idx in range(len(done)):\n",
    "            q_new = reward[idx]\n",
    "            if not done[idx]:\n",
    "                q_new = reward[idx] + self.gamma * torch.max(self.model(state_new[idx]))\n",
    "            target[idx][torch.argmax(action).item()] = q_new\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "\n",
    "def plot(scores, mean_scores):\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Number of Games')\n",
    "    plt.ylabel('Score')\n",
    "    plt.plot(scores)\n",
    "    plt.plot(mean_scores)\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.text(len(scores)-1, scores[-1], str(scores[-1]))\n",
    "    plt.text(len(mean_scores)-1, mean_scores[-1], str(mean_scores[-1]))\n",
    "    plt.show(block=False)\n",
    "    plt.pause(.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEM = 100000\n",
    "BATCH_SIZE = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0 #choose to explore or exploit policy \n",
    "        self.gamma = 0.9 # discounted factor\n",
    "        self.memory = deque(maxlen = MAX_MEM)\n",
    "        self.model = LinearQnet(11, 256, 3) #11 states, 256 hidden states, 3 action states\n",
    "        self.trainer = QTrainer(self.model, lr = LEARNING_RATE, gamma = self.gamma)\n",
    "\n",
    "        # TODO: model, trainer\n",
    "\n",
    "    def get_state(self, game: SnakeGameAI):\n",
    "        #There are 11 states in the game of snake, each is a boolean\n",
    "        # [danger sraight, danger right, danger left, \n",
    "        # direction left, right, up, down, \n",
    "        # food left, food right, food up, food down]\n",
    "        head = game.head\n",
    "        \n",
    "        point_left = Point(head.x - 20, head.y)\n",
    "        point_right = Point(head.x + 20, head.y)\n",
    "        point_up = Point(head.x, head.y - 20)\n",
    "        point_down = Point(head.x, head.y + 20)\n",
    "\n",
    "        # 4 points of direction\n",
    "        dir_left = game.direction == Direction.LEFT\n",
    "        dir_right = game.direction == Direction.RIGHT\n",
    "        dir_up = game.direction == Direction.UP\n",
    "        dir_down = game.direction == Direction.DOWN\n",
    "\n",
    "        state = [\n",
    "            # 3 state for danger state (straight, right, left)\n",
    "            #danger straight\n",
    "           (dir_up and game.is_collision(point_up)) or \n",
    "           (dir_right and game.is_collision(point_right)) or \n",
    "           (dir_down and game.is_collision(point_down)) or\n",
    "           (dir_left and game.is_collision(point_left)),\n",
    "           #danger right, this is relative to the direction of the snake\n",
    "           #but the point should be relative to world\n",
    "           #so danger right when going down is to its left point \n",
    "           (dir_up and game.is_collision(point_right)) or \n",
    "           (dir_right and game.is_collision(point_down)) or \n",
    "           (dir_down and game.is_collision(point_left)) or\n",
    "           (dir_left and game.is_collision(point_up)),\n",
    "           #same principle for danger left\n",
    "           (dir_up and game.is_collision(point_left)) or \n",
    "           (dir_right and game.is_collision(point_up)) or \n",
    "           (dir_down and game.is_collision(point_right)) or\n",
    "           (dir_left and game.is_collision(point_down)),\n",
    "\n",
    "            # 4 dim for move direction\n",
    "            dir_up, \n",
    "            dir_right, \n",
    "            dir_down, \n",
    "            dir_left, \n",
    "\n",
    "            # 4 dim for food location ( relative to the snake head)\n",
    "            # up, right, down, left\n",
    "            game.food.y < game.head.y, #up\n",
    "            game.food.x > game.head.x, #right\n",
    "            game.food.x < game.head.x, #left\n",
    "            game.food.y > game.head.y, #down\n",
    "        ]\n",
    "\n",
    "        # convert True false to 1, 0\n",
    "        return np.array(state, dtype = int)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            sample = random.sample(self.memory, BATCH_SIZE) #returns list of tuples\n",
    "        else:\n",
    "            sample = self.memory\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        self.epsilon = 80 - self.n_games\n",
    "        final_move = [0,0,0]\n",
    "        if random.randint(0,200) < self.epsilon:\n",
    "            move = random.randint(0,2)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            pred_action = self.model(torch.tensor(state, dtype=torch.float))\n",
    "            move = torch.argmax(pred_action).item()\n",
    "            final_move[move] = 1\n",
    "        return final_move\n",
    "\n",
    "def train():\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    total_score = 0\n",
    "    record = 0\n",
    "    agent = Agent()\n",
    "    game = SnakeGameAI()\n",
    "\n",
    "    while True:\n",
    "        # get old state\n",
    "        state_old = agent.get_state(game)\n",
    "        # get move\n",
    "        final_move = agent.get_action(state_old)\n",
    "        #perform move and get new state\n",
    "        reward, done, score = game.play_step(final_move)\n",
    "        state_new = agent.get_state(game)\n",
    "\n",
    "        #train short memory\n",
    "        agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        #remember\n",
    "        agent.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        if done:\n",
    "            # train long memory, plot result\n",
    "            game.reset()\n",
    "            agent.n_games += 1\n",
    "            agent.train_long_memory()\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                #save agent\n",
    "            \n",
    "            print('Game', agent.n_games, 'Score = ', score, ',Record=', record)\n",
    "\n",
    "            #TODO: plot\n",
    "            plot_scores.append(score)\n",
    "            total_score += score\n",
    "            mean_score = total_score / agent.n_games\n",
    "            plot_mean_scores.append(mean_score)\n",
    "\n",
    "            plot(plot_scores, plot_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train()\n",
      "Cell \u001b[0;32mIn[4], line 115\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m#perform move and get new state\u001b[39;00m\n\u001b[1;32m    114\u001b[0m reward, done, score \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mplay_step(final_move)\n\u001b[0;32m--> 115\u001b[0m state_new \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_state(game)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#train short memory\u001b[39;00m\n\u001b[1;32m    118\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain_short_memory(state_old, final_move, reward, state_new, done)\n",
      "Cell \u001b[0;32mIn[4], line 70\u001b[0m, in \u001b[0;36mAgent.get_state\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m     35\u001b[0m state \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# 3 state for danger state (straight, right, left)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#danger straight\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     game\u001b[38;5;241m.\u001b[39mfood\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m game\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39my, \u001b[38;5;66;03m#down\u001b[39;00m\n\u001b[1;32m     67\u001b[0m ]\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# convert True false to 1, 0\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
