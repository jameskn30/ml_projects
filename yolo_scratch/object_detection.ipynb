{"cells":[{"cell_type":"markdown","metadata":{"id":"rfnoi2f6Nhe3"},"source":["# Following this series:\n","https://www.youtube.com/watch?v=t-phGBfPEZ4&list=PLhhyoLH6Ijfw0TpCTVTNk42NN08H6UvNq"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1700061887296,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"},"user_tz":300},"id":"DCh1VxeJ1R_P"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","from torchinfo import summary\n","# from torchmetrics.detection import IntersectionOverUnion\n","from collections import *\n","# metrics from aladin peterson\n","# https://github.com/aladdinpersson/Machine-Learning-Collection\n","from utils.main import *\n","import os\n","import pandas as pd\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import v2, ToTensor\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1700058991909,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"},"user_tz":300},"id":"kKxBH_QvvZ0I"},"outputs":[],"source":["cnn_config = [\n","  # kernel, filter, stride, padding\n","  (7,64,2,3),\n","  # max pool\n","  \"M\",\n","  (3,192,1,1),\n","  \"M\",\n","  (1,128,1,0),\n","  (3,256,1,0),\n","  (3,256,1,0),\n","  (3,512,1,1),\n","  \"M\",\n","  #CNN block with 4 identical layers\n","  [(1,256,1,0), (3,512,1,1), 4],\n","  (1,512,1,0),\n","  (3,1024,1,1),\n","  \"M\",\n","  [(1,512,1,0), (3,1024, 1,1), 2],\n","  (3,1024,1,1),\n","  (3,1024,2,1),\n","  (3,1024,1,1),\n","  (3,1024,1,1),\n","]\n","\n","class CNNBlock(nn.Module):\n","  def __init__(self, out_channels, **kwargs):\n","    super(CNNBlock, self).__init__()\n","    #turn of bias to use BatchNorm\n","    self.conv = nn.LazyConv2d(out_channels, bias= False, **kwargs)\n","    self.batchnorm = nn.BatchNorm2d(num_features = out_channels)\n","    self.leaky_relu = nn.LeakyReLU(0.1)\n","\n","  def forward(self, X):\n","    return self.leaky_relu(self.batchnorm(self.conv(X)))\n","\n","class Yolov1(nn.Module):\n","  def __init__(self, **kwargs):\n","    super(Yolov1, self).__init__()\n","    self.cnn_config = cnn_config\n","    # self.in_channels = in_channels\n","    #pass in the cnn config to construct the model\n","    self.darknet = self._create_conv_layers(self.cnn_config)\n","    #fully connected layer creation\n","    self.fc = self._create_fc(**kwargs)\n","\n","  def forward(self, X):\n","    X = self.darknet(X)\n","    return self.fc(torch.flatten(X, start_dim=1))\n","\n","  def _create_conv_layers(self, config):\n","    layers = []\n","    # in_channels = self.in_channels\n","\n","    for x in config:\n","      #conv layer\n","      if type(x) == tuple:\n","        kernel, out_channels, stride, padding = x\n","        layers.append(CNNBlock(out_channels,\n","                               kernel_size = kernel, stride = stride, padding = padding))\n","      elif type(x) == str:\n","        layers.append(nn.MaxPool2d(kernel_size = (2,2), stride = (2,2)))\n","\n","      elif type(x) == list:\n","        conv1, conv2, repeats = x\n","        for _ in range(repeats):\n","          kernel1, out_channels1, stride1, padding1 = conv1\n","          layers.append(CNNBlock(out_channels1,\n","                                kernel_size = kernel1, stride = stride1, padding = padding1))\n","\n","          kernel2, out_channels2, stride2, padding2 = conv2\n","          layers.append(CNNBlock(out_channels2,\n","                                kernel_size = kernel2, stride = stride2, padding = padding2))\n","          #according to the paper, for each repeated block, we take output channel of conv2\n","          #and feed it as in channel to the next block\n","          # in_channels = out_channels2\n","    #unpack a list [a,b,c,d,...] into a,b,c,d,...\n","    return nn.Sequential(*layers)\n","\n","\n","  def _create_fc(self, grid_size, num_boxes, num_classes):\n","    # pred vector should look like [c1,c2,...cN, p1, x1,y1,w1,h2, p2, x2,y2,w2,h2]\n","    S, B, C = grid_size, num_boxes, num_classes\n","\n","    return nn.Sequential(\n","        nn.Flatten(),\n","        nn.LazyLinear(496), # original paper is 4096, reduce it to reduce training resources\n","        nn.Dropout(0.5),\n","        nn.LeakyReLU(0.1),\n","        #each cell has # of classes + # of boxes * 5 (5 because it's p,x,y,w,h)\n","        nn.Linear(496, S * S * (C + B * 5)), #Reshape to (S*S * 30)\n","    )\n","\n"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5822,"status":"ok","timestamp":1700058997714,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"},"user_tz":300},"id":"6Uwij8p01mot","outputId":"caa4c9e0-40a0-41f0-89f4-28d7c00bb2d2"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]},{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Yolov1                                   [2, 1470]                 --\n","├─Sequential: 1-1                        [2, 1024, 7, 7]           --\n","│    └─CNNBlock: 2-1                     [2, 64, 224, 224]         --\n","│    │    └─Conv2d: 3-1                  [2, 64, 224, 224]         9,408\n","│    │    └─BatchNorm2d: 3-2             [2, 64, 224, 224]         128\n","│    │    └─LeakyReLU: 3-3               [2, 64, 224, 224]         --\n","│    └─MaxPool2d: 2-2                    [2, 64, 112, 112]         --\n","│    └─CNNBlock: 2-3                     [2, 192, 112, 112]        --\n","│    │    └─Conv2d: 3-4                  [2, 192, 112, 112]        110,592\n","│    │    └─BatchNorm2d: 3-5             [2, 192, 112, 112]        384\n","│    │    └─LeakyReLU: 3-6               [2, 192, 112, 112]        --\n","│    └─MaxPool2d: 2-4                    [2, 192, 56, 56]          --\n","│    └─CNNBlock: 2-5                     [2, 128, 56, 56]          --\n","│    │    └─Conv2d: 3-7                  [2, 128, 56, 56]          24,576\n","│    │    └─BatchNorm2d: 3-8             [2, 128, 56, 56]          256\n","│    │    └─LeakyReLU: 3-9               [2, 128, 56, 56]          --\n","│    └─CNNBlock: 2-6                     [2, 256, 54, 54]          --\n","│    │    └─Conv2d: 3-10                 [2, 256, 54, 54]          294,912\n","│    │    └─BatchNorm2d: 3-11            [2, 256, 54, 54]          512\n","│    │    └─LeakyReLU: 3-12              [2, 256, 54, 54]          --\n","│    └─CNNBlock: 2-7                     [2, 256, 52, 52]          --\n","│    │    └─Conv2d: 3-13                 [2, 256, 52, 52]          589,824\n","│    │    └─BatchNorm2d: 3-14            [2, 256, 52, 52]          512\n","│    │    └─LeakyReLU: 3-15              [2, 256, 52, 52]          --\n","│    └─CNNBlock: 2-8                     [2, 512, 52, 52]          --\n","│    │    └─Conv2d: 3-16                 [2, 512, 52, 52]          1,179,648\n","│    │    └─BatchNorm2d: 3-17            [2, 512, 52, 52]          1,024\n","│    │    └─LeakyReLU: 3-18              [2, 512, 52, 52]          --\n","│    └─MaxPool2d: 2-9                    [2, 512, 26, 26]          --\n","│    └─CNNBlock: 2-10                    [2, 256, 26, 26]          --\n","│    │    └─Conv2d: 3-19                 [2, 256, 26, 26]          131,072\n","│    │    └─BatchNorm2d: 3-20            [2, 256, 26, 26]          512\n","│    │    └─LeakyReLU: 3-21              [2, 256, 26, 26]          --\n","│    └─CNNBlock: 2-11                    [2, 512, 26, 26]          --\n","│    │    └─Conv2d: 3-22                 [2, 512, 26, 26]          1,179,648\n","│    │    └─BatchNorm2d: 3-23            [2, 512, 26, 26]          1,024\n","│    │    └─LeakyReLU: 3-24              [2, 512, 26, 26]          --\n","│    └─CNNBlock: 2-12                    [2, 256, 26, 26]          --\n","│    │    └─Conv2d: 3-25                 [2, 256, 26, 26]          131,072\n","│    │    └─BatchNorm2d: 3-26            [2, 256, 26, 26]          512\n","│    │    └─LeakyReLU: 3-27              [2, 256, 26, 26]          --\n","│    └─CNNBlock: 2-13                    [2, 512, 26, 26]          --\n","│    │    └─Conv2d: 3-28                 [2, 512, 26, 26]          1,179,648\n","│    │    └─BatchNorm2d: 3-29            [2, 512, 26, 26]          1,024\n","│    │    └─LeakyReLU: 3-30              [2, 512, 26, 26]          --\n","│    └─CNNBlock: 2-14                    [2, 256, 26, 26]          --\n","│    │    └─Conv2d: 3-31                 [2, 256, 26, 26]          131,072\n","│    │    └─BatchNorm2d: 3-32            [2, 256, 26, 26]          512\n","│    │    └─LeakyReLU: 3-33              [2, 256, 26, 26]          --\n","│    └─CNNBlock: 2-15                    [2, 512, 26, 26]          --\n","│    │    └─Conv2d: 3-34                 [2, 512, 26, 26]          1,179,648\n","│    │    └─BatchNorm2d: 3-35            [2, 512, 26, 26]          1,024\n","│    │    └─LeakyReLU: 3-36              [2, 512, 26, 26]          --\n","│    └─CNNBlock: 2-16                    [2, 256, 26, 26]          --\n","│    │    └─Conv2d: 3-37                 [2, 256, 26, 26]          131,072\n","│    │    └─BatchNorm2d: 3-38            [2, 256, 26, 26]          512\n","│    │    └─LeakyReLU: 3-39              [2, 256, 26, 26]          --\n","│    └─CNNBlock: 2-17                    [2, 512, 26, 26]          --\n","│    │    └─Conv2d: 3-40                 [2, 512, 26, 26]          1,179,648\n","│    │    └─BatchNorm2d: 3-41            [2, 512, 26, 26]          1,024\n","│    │    └─LeakyReLU: 3-42              [2, 512, 26, 26]          --\n","│    └─CNNBlock: 2-18                    [2, 512, 26, 26]          --\n","│    │    └─Conv2d: 3-43                 [2, 512, 26, 26]          262,144\n","│    │    └─BatchNorm2d: 3-44            [2, 512, 26, 26]          1,024\n","│    │    └─LeakyReLU: 3-45              [2, 512, 26, 26]          --\n","│    └─CNNBlock: 2-19                    [2, 1024, 26, 26]         --\n","│    │    └─Conv2d: 3-46                 [2, 1024, 26, 26]         4,718,592\n","│    │    └─BatchNorm2d: 3-47            [2, 1024, 26, 26]         2,048\n","│    │    └─LeakyReLU: 3-48              [2, 1024, 26, 26]         --\n","│    └─MaxPool2d: 2-20                   [2, 1024, 13, 13]         --\n","│    └─CNNBlock: 2-21                    [2, 512, 13, 13]          --\n","│    │    └─Conv2d: 3-49                 [2, 512, 13, 13]          524,288\n","│    │    └─BatchNorm2d: 3-50            [2, 512, 13, 13]          1,024\n","│    │    └─LeakyReLU: 3-51              [2, 512, 13, 13]          --\n","│    └─CNNBlock: 2-22                    [2, 1024, 13, 13]         --\n","│    │    └─Conv2d: 3-52                 [2, 1024, 13, 13]         4,718,592\n","│    │    └─BatchNorm2d: 3-53            [2, 1024, 13, 13]         2,048\n","│    │    └─LeakyReLU: 3-54              [2, 1024, 13, 13]         --\n","│    └─CNNBlock: 2-23                    [2, 512, 13, 13]          --\n","│    │    └─Conv2d: 3-55                 [2, 512, 13, 13]          524,288\n","│    │    └─BatchNorm2d: 3-56            [2, 512, 13, 13]          1,024\n","│    │    └─LeakyReLU: 3-57              [2, 512, 13, 13]          --\n","│    └─CNNBlock: 2-24                    [2, 1024, 13, 13]         --\n","│    │    └─Conv2d: 3-58                 [2, 1024, 13, 13]         4,718,592\n","│    │    └─BatchNorm2d: 3-59            [2, 1024, 13, 13]         2,048\n","│    │    └─LeakyReLU: 3-60              [2, 1024, 13, 13]         --\n","│    └─CNNBlock: 2-25                    [2, 1024, 13, 13]         --\n","│    │    └─Conv2d: 3-61                 [2, 1024, 13, 13]         9,437,184\n","│    │    └─BatchNorm2d: 3-62            [2, 1024, 13, 13]         2,048\n","│    │    └─LeakyReLU: 3-63              [2, 1024, 13, 13]         --\n","│    └─CNNBlock: 2-26                    [2, 1024, 7, 7]           --\n","│    │    └─Conv2d: 3-64                 [2, 1024, 7, 7]           9,437,184\n","│    │    └─BatchNorm2d: 3-65            [2, 1024, 7, 7]           2,048\n","│    │    └─LeakyReLU: 3-66              [2, 1024, 7, 7]           --\n","│    └─CNNBlock: 2-27                    [2, 1024, 7, 7]           --\n","│    │    └─Conv2d: 3-67                 [2, 1024, 7, 7]           9,437,184\n","│    │    └─BatchNorm2d: 3-68            [2, 1024, 7, 7]           2,048\n","│    │    └─LeakyReLU: 3-69              [2, 1024, 7, 7]           --\n","│    └─CNNBlock: 2-28                    [2, 1024, 7, 7]           --\n","│    │    └─Conv2d: 3-70                 [2, 1024, 7, 7]           9,437,184\n","│    │    └─BatchNorm2d: 3-71            [2, 1024, 7, 7]           2,048\n","│    │    └─LeakyReLU: 3-72              [2, 1024, 7, 7]           --\n","├─Sequential: 1-2                        [2, 1470]                 --\n","│    └─Flatten: 2-29                     [2, 50176]                --\n","│    └─Linear: 2-30                      [2, 496]                  24,887,792\n","│    └─Dropout: 2-31                     [2, 496]                  --\n","│    └─LeakyReLU: 2-32                   [2, 496]                  --\n","│    └─Linear: 2-33                      [2, 1470]                 730,590\n","==========================================================================================\n","Total params: 86,311,822\n","Trainable params: 86,311,822\n","Non-trainable params: 0\n","Total mult-adds (G): 38.54\n","==========================================================================================\n","Input size (MB): 4.82\n","Forward/backward pass size (MB): 409.70\n","Params size (MB): 345.25\n","Estimated Total Size (MB): 759.76\n","=========================================================================================="]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["S = 7\n","B = 2\n","C = 20\n","\n","model = Yolov1(grid_size = S, num_boxes = B, num_classes = C)\n","summary(model, input_size=(2,3,448,448))"]},{"cell_type":"markdown","metadata":{},"source":["# Loss function\n","check formula in the paper"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"LAgBF4CtHJXF"},"outputs":[],"source":["# The idea of this is each prediction gives 2 boxes\n","# There's only 1 box for target\n","# then get IoU of these 2 pred boxes on 1 target box\n","\n","class YoloLoss(nn.Module):\n","  def __init__(self, S= 7,B = 2, C = 30):\n","    super(YoloLoss, self).__init__()\n","\n","    self.mse = nn.MSELoss(reduction = 'sum')\n","    self.S = S\n","    self.B = B\n","    self.C = C\n","    self.lambda_noobj = 0.5\n","    self.lambda_coord = 5\n","\n","  def forward(self, preds, target):\n","    #Input shape = 2 * 1470 ( from summary above)\n","    # Reshape to (-1, S, S, C + B * 5)\n","    #Preds = [c1,c2,...c20, p1, x1, y2, w1, h1, p2, x2, y2, w2, h2]\n","\n","    preds = preds.reshape(-1, self.S, self.S, self.C + self.B * 5)\n","    box1 = preds[...]\n","    # 21:25 is the first box x1,y1,w1,h1\n","    iou_box1 = intersection_over_union(preds[..., 21:25], target[..., 21:25])\n","    # 26:30 is the 2nd box x2,y2,w2,h2\n","    iou_box2 = intersection_over_union(preds[..., 26:30], target[..., 21:25])\n","\n","    #torch.squeeze: insert new dimension to that specify position\n","    ious = torch.cat([iou_box1.unsqueeze(0), iou_box2.unsqueeze(0)], dim = 0)\n","    iou_max, bestbox = torch.max(ious, dim = 0)\n","    #exists box, 1 or 0\n","    exists_box = target[..., 20].unsqueeze(3)\n","    #box coords\n","\n","    box_preds = exists_box * (bestbox * (preds[..., 26:30]) + \\\n","                              (1 - bestbox) * (preds[..., 21:25]))\n","    box_targets = exists_box * target[..., 21 : 25]\n","\n","    #the 1e-6 is for numerical stability\n","    # torch.sign([-1,-0.5, 1, 0.5, 0]) --> [-1,-1, 1, 1, 0]\n","    box_preds[..., 2:4] = torch.sign(box_preds[..., 2:4]) * \\\n","      torch.sqrt(torch.abs(box_preds[..., 2:4] + 1e-6))\n","\n","    box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n","\n","    #Flatten the first 3 dims, input (N, S, S, 4) --> (N * S * S, 4)\n","    box_loss = self.mse(\n","         torch.flatten(box_preds, end_dim = -2),\n","         torch.flatten(box_targets, end_dim = -2),\n","    )\n","\n","    #object loss\n","    # pred box shape = (N * S * S, 1)\n","    pred_box = (\n","        bestbox * preds[..., 25:26] + (1 - bestbox) * preds[..., 20:21]\n","    )\n","\n","    object_loss = self.mse(\n","      torch.flatten(exists_box * pred_box),\n","      torch.flatten(exists_box * target[..., 20:21]),\n","    )\n","\n","    #no object loss\n","    # (N,S,S,1) --> (N, S * S)\n","    noobject_loss = self.mse(\n","      torch.flatten((1 - exists_box) * preds[..., 20:21], start_dim=1),\n","      torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n","    )\n","    noobject_loss += self.mse(\n","      torch.flatten((1 - exists_box) * preds[..., 25:26], start_dim=1),\n","      torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n","    )\n","\n","    #class loss\n","    # (N, S, S, 20) --> (N *S * S, 20)\n","    class_loss = self.mse(\n","      torch.flatten(exists_box * preds[..., :20], end_dim = -2),\n","      torch.flatten(exists_box * target[..., :20], end_dim = -2)\n","    )\n","\n","    loss = (\n","      self.lambda_coord * box_loss\\\n","      + object_loss\n","      + self.lambda_noobj * noobject_loss\n","      + class_loss\n","    )\n","\n","    return loss\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the dataset"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n","  warnings.warn(\n"]}],"source":["class VOCDataset(Dataset):\n","    def __init__(self, csv_file, img_dir, label_dir, S = 7, B = 2, C = 20, transforms = None):\n","        self.annotations = pd.read_csv(csv_file)\n","        self.img_dir = img_dir\n","        self.label_dir = label_dir\n","        self.S = S\n","        self.B = B\n","        self.C = C\n","\n","        if transforms == None:\n","            transforms = v2.Compose([\n","                v2.Resize((448,448)),\n","                v2.ToTensor()\n","            ])\n","\n","        self.transforms = transforms\n","    \n","    def __len__(self):\n","        return len(self.annotations)\n","    \n","    def __getitem__(self, index):\n","        image_path = os.path.join(self.img_dir, self.annotations.iloc[index,0])\n","\n","        #NOTE: the x,y,w,h in label is relative to the entire image, not each cells\n","        #we have to convert it to relative to each cell based on our configuration\n","        label_path = os.path.join(self.label_dir, self.annotations.iloc[index,1])\n","        boxes = []\n","\n","        image = Image.open(image_path)\n","\n","        #Load box labels\n","        with open(label_path) as file:\n","            for line in file.readlines():\n","                line = line.strip()\n","                class_label, x,y,w,h = line.split()\n","                class_label = int(class_label)\n","                x = float(x)\n","                y = float(y)\n","                w = float(w)\n","                h = float(h)\n","                boxes.append((class_label, x,y,w,h))\n","        \n","        #We not using transforms in this tutorial\n","        #This is for torch transforms\n","        # boxes = torch.tensor(boxes)\n","\n","        #Converting x,y,w,h relateive to image to x,y,w,h relative to each cell\n","        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n","        for box in boxes:\n","            #NOTE: Confusing!!! try to understand these 2 lines\n","            class_label, x,y,w,h = box\n","            i, j = int(self.S * y) , int(self.S * x)\n","            x_cell, y_cell = self.S * x - j, self.S * y - i\n","            width_cell, height_cell = w * self.S, h*self.S\n","\n","            if label_matrix[i, j, 20] == 0:\n","                label_matrix[i, j,20] = 1\n","                box_coords = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n","                label_matrix[i,j,21:25] = box_coords\n","                label_matrix[i,j,class_label] = 1\n","        #Final label matrix format\n","        #(0,0,0..,1,0,0,0,1,x,y,w,h,0,0,0,0,0)\n","        # 5th position is class 6, set to 1\n","        # 20th position is there's an object in this box, set to 1\n","        # 21 -> 24th position is box coords relative to cell\n","        image = self.transforms(image)\n","        \n","        return image, label_matrix\n","        \n","VOC_PATH = './data/'\n","VOC_IMAGE_PATH = './data/images/'\n","VOC_LABEL_PATH = './data/labels/'\n","\n","TRAIN_VOC_CSV = os.path.join(VOC_PATH, 'train.csv')\n","voc = VOCDataset(TRAIN_VOC_CSV, VOC_IMAGE_PATH, VOC_LABEL_PATH)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0000, 1.0000, 0.4870, 0.9940, 5.0260, 5.8859, 0.0000, 0.0000,\n","        0.0000, 0.0000, 0.0000])\n","torch.Size([7, 7, 30])\n"]}],"source":["image, label = voc[0]\n","print(label[3][4])\n","print(label.shape)\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0.2588, 0.1373, 0.0745,  ..., 0.2039, 0.2118, 0.1373],\n","         [0.5765, 0.4471, 0.0588,  ..., 0.2196, 0.3098, 0.2078],\n","         [0.5333, 0.4980, 0.0784,  ..., 0.2157, 0.2627, 0.2314],\n","         ...,\n","         [0.3059, 0.3765, 0.2431,  ..., 0.0588, 0.0784, 0.0784],\n","         [0.3176, 0.4196, 0.2118,  ..., 0.0588, 0.0902, 0.0824],\n","         [0.3216, 0.3373, 0.1765,  ..., 0.0549, 0.0824, 0.0745]],\n","\n","        [[0.1059, 0.0706, 0.1020,  ..., 0.3608, 0.3373, 0.2392],\n","         [0.5020, 0.4275, 0.0941,  ..., 0.3255, 0.4078, 0.3059],\n","         [0.5608, 0.5373, 0.1255,  ..., 0.2745, 0.3373, 0.3294],\n","         ...,\n","         [0.3804, 0.4471, 0.3255,  ..., 0.0667, 0.0863, 0.0863],\n","         [0.3686, 0.4824, 0.2902,  ..., 0.0667, 0.0980, 0.0902],\n","         [0.3529, 0.3961, 0.2549,  ..., 0.0627, 0.0902, 0.0824]],\n","\n","        [[0.0824, 0.0353, 0.0549,  ..., 0.2471, 0.2235, 0.1137],\n","         [0.4667, 0.3843, 0.0431,  ..., 0.2196, 0.2941, 0.1765],\n","         [0.5059, 0.4784, 0.0667,  ..., 0.1686, 0.2275, 0.1922],\n","         ...,\n","         [0.1373, 0.2392, 0.1373,  ..., 0.0627, 0.0824, 0.0824],\n","         [0.1412, 0.2902, 0.1216,  ..., 0.0627, 0.0941, 0.0863],\n","         [0.1451, 0.2196, 0.0980,  ..., 0.0588, 0.0863, 0.0784]]])\n","torch.Size([3, 448, 448])\n"]}],"source":["#Transformed \n","print(image)\n","print(image.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Training and evaluate"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["# Hyperparams\n","LEARNING_RATE = 2e-5\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","BATCH_SIZE = 16\n","WEIGHT_DECAY = 0\n","EPOCHS = 10\n","NUM_WORKERS = 2\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","LOAD_MODEL_FILE = 'model.h5'\n","VOC_PATH = './data/'\n","VOC_IMAGE_PATH = './data/images/'\n","VOC_LABEL_PATH = './data/labels/'\n","TRAIN_VOC_CSV = os.path.join(VOC_PATH, 'train.csv')\n"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["def train(model, train_dataloader, optimizer, criterion):\n","    loop = tqdm(train_dataloader)\n","\n","    mean_loss =[]\n","\n","    for batch, (x,y) in enumerate(train_dataloader):\n","        x = x.to(DEVICE).type(torch.FloatTensor)\n","        y = y.to(DEVICE)\n","        out = model(x)\n","\n","        optimizer.zero_grad()\n","        loss = criterion(out, y)\n","        mean_loss.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        #Update the progress bar\n","        loop.set_postfix(loss = loss.item())\n","    \n","    print(f'mean loss = {sum(mean_loss) / len(mean_loss)}')\n","\n"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]}],"source":["#define model\n","S = 7\n","B = 2\n","C = 20\n","\n","model = Yolov1(grid_size = S, num_boxes = B, num_classes = C)"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n","  warnings.warn(\n","\n","\n","\n","\n","\n","\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"]},{"ename":"RuntimeError","evalue":"The size of tensor a (12) must match the size of tensor b (16) at non-singleton dimension 0","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32me:\\ML\\ml_projects\\yolo_scratch\\object_detection.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(dataset\u001b[39m=\u001b[39mtrain_dataset, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     train(model,train_dataloader, optim, loss_fn)\n","\u001b[1;32me:\\ML\\ml_projects\\yolo_scratch\\object_detection.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m out \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m mean_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n","File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32me:\\ML\\ml_projects\\yolo_scratch\\object_detection.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m box1 \u001b[39m=\u001b[39m preds[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# 21:25 is the first box x1,y1,w1,h1\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m iou_box1 \u001b[39m=\u001b[39m intersection_over_union(preds[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m21\u001b[39;49m:\u001b[39m25\u001b[39;49m], target[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m21\u001b[39;49m:\u001b[39m25\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# 26:30 is the 2nd box x2,y2,w2,h2\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/ml_projects/yolo_scratch/object_detection.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m iou_box2 \u001b[39m=\u001b[39m intersection_over_union(preds[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m26\u001b[39m:\u001b[39m30\u001b[39m], target[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m21\u001b[39m:\u001b[39m25\u001b[39m])\n","File \u001b[1;32me:\\ML\\ml_projects\\yolo_scratch\\utils\\main.py:40\u001b[0m, in \u001b[0;36mintersection_over_union\u001b[1;34m(boxes_preds, boxes_labels, box_format)\u001b[0m\n\u001b[0;32m     37\u001b[0m     box2_x2 \u001b[39m=\u001b[39m boxes_labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m2\u001b[39m:\u001b[39m3\u001b[39m]\n\u001b[0;32m     38\u001b[0m     box2_y2 \u001b[39m=\u001b[39m boxes_labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m:\u001b[39m4\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmax(box1_x1, box2_x1)\n\u001b[0;32m     41\u001b[0m y1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(box1_y1, box2_y1)\n\u001b[0;32m     42\u001b[0m x2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(box1_x2, box2_x2)\n","\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (16) at non-singleton dimension 0"]}],"source":["optim = torch.optim.Adam(model.parameters(), \n","                         lr = LEARNING_RATE, \n","                         weight_decay = WEIGHT_DECAY\n","                         )\n","loss_fn = YoloLoss()\n","TRAIN_VOC_CSV = os.path.join(VOC_PATH, '100examples.csv')\n","train_dataset = VOCDataset(TRAIN_VOC_CSV, VOC_IMAGE_PATH, VOC_LABEL_PATH)\n","\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, drop_last=True)\n","\n","for epoch in range(EPOCHS):\n","    train(model,train_dataloader, optim, loss_fn)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700064342737,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"},"user_tz":300},"id":"zxJqm-aXYB9z"},"outputs":[],"source":["# def IoU(box_preds, box_labels, box_format = \"midpoint\"):\n","\n","#   if box_format == 'midpoint':\n","#     box1_x1 = box_preds[..., 0:1] - box_preds[..., 2:3] / 2\n","#     box1_y1 = box_preds[..., 1:2] - box_preds[..., 3:4] / 2\n","#     box1_x2 = box_preds[..., 0:1] + box_preds[..., 2:3] / 2\n","#     box1_y2 = box_preds[..., 1:2] + box_preds[..., 3:4] / 2\n","\n","#     box2_x1 = box_labels[..., 0:1] - box_labels[..., 2:3] / 2\n","#     box2_y1 = box_labels[..., 1:2] - box_labels[..., 3:4] / 2\n","#     box2_x2 = box_labels[..., 0:1] + box_labels[..., 2:3] / 2\n","#     box2_y2 = box_labels[..., 1:2] + box_labels[..., 3:4] / 2\n","#   else:\n","#     box1_x1 = box_preds[..., 0:1]\n","#     box1_y1 = box_preds[..., 1:2]\n","#     box1_x2 = box_preds[..., 2:3]\n","#     box1_y2 = box_preds[..., 3:4]\n","#     box2_x1 = box_labels[..., 0:1]\n","#     box2_y1 = box_labels[..., 1:2]\n","#     box2_x2 = box_labels[..., 2:3]\n","#     box2_y2 = box_labels[..., 3:4]\n","\n","#   x1 = torch.max(box1_x1, box2_x1)\n","#   y1 = torch.max(box1_y1, box2_y1)\n","#   x2 = torch.min(box1_x2, box2_x2)\n","#   y2 = torch.min(box2_y2, box2_y2)\n","\n","#   #if x2 - x1 < 0 or y2 - y1 < 0, set intersection to 0\n","#   intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n","\n","#   box1_area = abs((box1_x2 - box1_x1) * (box1_y1 - box1_y2))\n","#   box2_area = abs((box2_x2 - box2_x1) * (box2_y1 - box2_y2))\n","\n","#   return intersection / (box1_area + box2_area)\n"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1700064349246,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"},"user_tz":300},"id":"U6CNWH-BiQXg"},"outputs":[],"source":["# def no_max_supression(bbox, iou_threshold, threshold, box_format = 'midpoint'):\n","#   #pred format (1, 0.9, x1, y1, x2, y2)\n","#   # where 1 is the 0,1 value,1 means there's an object in this bbox, else 0\n","#   # 0.9 is the probability of this box\n","#   assert type(bbox) == list\n","#   bbox = [box for box in bbox if box[1] > threshold]\n","#   bbox = sorted(bbox, key = lambda x: x[1], reverse = True)\n","#   bbox_after_nms = []\n","\n","#   while bbox:\n","#     chosen_box = bbox.pop(0)\n","\n","#     bbox = [\n","#         box for box in bbox if box[0] != chosen_box[0]\\\n","#         or IoU(torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format = box_format) < iou_threshold\n","#     ]\n","\n","#     bbox_after_nms.append(chosen_box)\n","\n","#   return bbox_after_nms\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700064807230,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"},"user_tz":300},"id":"duTRWFMdkt51"},"outputs":[],"source":["# # Mean Average Precision as metric\n","# def mAP(pred_boxes, true_boxes, iou_threshold =0.5, box_format = 'midpoint', num_classes = 20):\n","#   average_precisions = []\n","#   #for numerical stability in float\n","#   epsilon = 1e-6\n","\n","#   #pred boxes(list): [[train_idx, class_pred, prob_score, x1, y1, x2, y2], ....]\n","#   #same for true boxes\n","#   for c in range(num_classes):\n","#     detections = []\n","#     ground_truths = []\n","\n","#     #only choose pred and true for that class\n","#     for pred_box in pred_boxes:\n","#       if pred_box[1] == c:\n","#         detections.append(pred_box)\n","#     for true_box in true_boxes:\n","#       if true_box[1] == c:\n","#         ground_truths.append(true_box)\n","\n","#     #img0 has 3 bboxes\n","#     #img1 has 5 bboxes\n","#     #convert to dictionary: amount_bboxes = {0:3, 1:5}\n","#     amount_bboxes = Counter([gt[0] for gt in ground_truths])\n","\n","#     #convert to dictionary with tensors:\n","#     #amount_bboxes = {0: tensor([0,0,0]), 1:tensor([0,0,0,0,0])}\n","#     #we're doing this because we gonna mark only 1 box in that image as true\n","#     #the other images are FP\n","#     for key, val in amount_bboxes.item():\n","#       amount_bboxes[key] = torch.zeros(val)\n","\n","#     detections.sort(key = lambda x: x[2], reverse = True)\n","#     TP = torch.zeros(len(detections))\n","#     FP = torch.zeros(len(detections))\n","\n","#     total_true_boxes = len(ground_truths)\n","\n","#     for detection_idx, detection in enumerate(detections):\n","#       #get the ground truth with the same id with the detected box\n","#       ground_truth_img = [ bbox for bbox in ground_truths if bbox[0] == detection[0]]\n","\n","#       num_ground_truths = len(ground_truth_img)\n","#       best_iou = 0\n","#       best_gt_idx = 0\n","\n","#       for idx, gt in enumerate(ground_truth_img):\n","#         iou = IoU(torch.tensor(detection[3:]), torch.tensor(ground_truths[3:]), box_format= box_format)\n","\n","#         if iou > best_iou:\n","#           best_iou = iou\n","#           best_gt_idx = idx\n","\n","#       if best_iou > iou_threshold:\n","#         if amount_bboxes[detection[0]][best_gt_idx] == 0:\n","#           TP[detection_idx] = 1\n","#           amount_bboxes[detection[0]][best_gt_idx] = 1\n","#         else:\n","#           FP[detection_idx] = 1\n","#       else:\n","#           FP[detection_idx] = 1\n","\n","#       # [1,1,0,1,1,0] --> [1,2,2,3,4,4]\n","#       TP_cumsum = torch.cumsum(TP, dim = 0)\n","#       FP_cumsum = torch.cumsum(FP, dim = 0)\n","\n","#       recalls = TP_cumsum / (total_true_boxes + epsilon)\n","#       precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n","#       precisions = torch.cat(torch.tensor([1]), precisions)\n","\n","#       recalls = torch.cat((torch.tensor([0])), recalls)\n","\n","#       average_precisions.append(torch.trapz(precisions, recalls))\n","\n","#     return  sum(average_precisions) / len(average_precisions)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNcI6xs3VG6o3MEVB1qsNdC","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
